{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4537092d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/05/12 18:58:02 INFO SparkContext: Running Spark version 3.2.0\n",
      "22/05/12 18:58:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/12 18:58:02 INFO ResourceUtils: ==============================================================\n",
      "22/05/12 18:58:02 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/05/12 18:58:02 INFO ResourceUtils: ==============================================================\n",
      "22/05/12 18:58:02 INFO SparkContext: Submitted application: 0eac3d19-f007-47d4-a18f-55ebc593121d\n",
      "22/05/12 18:58:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/05/12 18:58:02 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/05/12 18:58:02 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/05/12 18:58:02 INFO SecurityManager: Changing view acls to: oscar\n",
      "22/05/12 18:58:02 INFO SecurityManager: Changing modify acls to: oscar\n",
      "22/05/12 18:58:02 INFO SecurityManager: Changing view acls groups to: \n",
      "22/05/12 18:58:02 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/05/12 18:58:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oscar); groups with view permissions: Set(); users  with modify permissions: Set(oscar); groups with modify permissions: Set()\n",
      "22/05/12 18:58:02 INFO Utils: Successfully started service 'sparkDriver' on port 40165.\n",
      "22/05/12 18:58:02 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/05/12 18:58:02 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/05/12 18:58:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/05/12 18:58:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/05/12 18:58:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/12 18:58:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1b8d550c-d4e3-4d77-a36d-9fa63c433e1d\n",
      "22/05/12 18:58:03 INFO MemoryStore: MemoryStore started with capacity 4.0 GiB\n",
      "22/05/12 18:58:03 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/05/12 18:58:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/05/12 18:58:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://netrunner:4040\n",
      "22/05/12 18:58:03 INFO Executor: Starting executor ID driver on host netrunner\n",
      "22/05/12 18:58:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34759.\n",
      "22/05/12 18:58:03 INFO NettyBlockTransferService: Server created on netrunner:34759\n",
      "22/05/12 18:58:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/05/12 18:58:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, netrunner, 34759, None)\n",
      "22/05/12 18:58:03 INFO BlockManagerMasterEndpoint: Registering block manager netrunner:34759 with 4.0 GiB RAM, BlockManagerId(driver, netrunner, 34759, None)\n",
      "22/05/12 18:58:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, netrunner, 34759, None)\n",
      "22/05/12 18:58:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, netrunner, 34759, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5a69721d\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.2.0` // Or use any other 2.x version here\n",
    "import $ivy.`sh.almond::almond-spark:0.10.1`\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28837acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfiles\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m\"circuits\"\u001b[39m,\n",
       "  \u001b[32m\"constructor_results\"\u001b[39m,\n",
       "  \u001b[32m\"constructors\"\u001b[39m,\n",
       "  \u001b[32m\"constructor_standings\"\u001b[39m,\n",
       "  \u001b[32m\"drivers\"\u001b[39m,\n",
       "  \u001b[32m\"driver_standings\"\u001b[39m,\n",
       "  \u001b[32m\"drivers_constr_season\"\u001b[39m,\n",
       "  \u001b[32m\"lap_times\"\u001b[39m,\n",
       "  \u001b[32m\"pit_stops\"\u001b[39m,\n",
       "  \u001b[32m\"qualifying\"\u001b[39m,\n",
       "  \u001b[32m\"races\"\u001b[39m,\n",
       "  \u001b[32m\"results\"\u001b[39m,\n",
       "  \u001b[32m\"seasons\"\u001b[39m,\n",
       "  \u001b[32m\"status\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val files = List(\"circuits\", \"constructor_results\", \"constructors\", \"constructor_standings\",\n",
    "                 \"drivers\", \"driver_standings\", \"drivers_constr_season\", \"lap_times\", \"pit_stops\",\n",
    "                 \"qualifying\", \"races\", \"results\", \"seasons\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5670899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpartCol\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m] = \u001b[33mMap\u001b[39m(\n",
       "  \u001b[32m\"pit_stops\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"drivers\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"races\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"constructor_results\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"constructors\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"driver_standings\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"results\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"status\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"drivers_constr_season\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"lap_times\"\u001b[39m -> \u001b[32m\"driverId\"\u001b[39m,\n",
       "  \u001b[32m\"circuits\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"constructor_standings\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"qualifying\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m,\n",
       "  \u001b[32m\"seasons\"\u001b[39m -> \u001b[32m\"noPartitionCol\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val partCol = Map(\n",
    "    \"circuits\" -> \"noPartitionCol\",\n",
    "    \"constructor_results\" -> \"noPartitionCol\",\n",
    "    \"constructors\" -> \"noPartitionCol\",\n",
    "    \"constructor_standings\" -> \"noPartitionCol\",\n",
    "    \"drivers\" -> \"noPartitionCol\",\n",
    "    \"driver_standings\" -> \"noPartitionCol\",\n",
    "    \"drivers_constr_season\" -> \"noPartitionCol\",\n",
    "    \"lap_times\" -> \"driverId\",\n",
    "    \"pit_stops\" -> \"noPartitionCol\",\n",
    "    \"qualifying\" -> \"noPartitionCol\",\n",
    "    \"races\" -> \"noPartitionCol\",\n",
    "    \"results\" -> \"noPartitionCol\",\n",
    "    \"seasons\" -> \"noPartitionCol\",\n",
    "    \"status\" -> \"noPartitionCol\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "622177e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtoParquet\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toParquet(tableName: String, partitionCol: String): Unit = {\n",
    "    val ini = spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .load(s\"../data/$tableName.csv\")\n",
    "    \n",
    "    if (partitionCol == \"noPartitionCol\") {\n",
    "        ini.write.mode(\"overwrite\")\n",
    "            .parquet(s\"../data/parquet/$tableName.parquet\")\n",
    "    } else {\n",
    "        ini.repartition(col(partitionCol))\n",
    "            .write.mode(\"overwrite\")\n",
    "            .partitionBy(partitionCol)\n",
    "            .parquet(s\"../data/parquet/$tableName.parquet\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa9dbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtoParquetNoPart\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toParquetNoPart(tableName: String): Unit = {\n",
    "    spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .load(s\"../data/$tableName.csv\")\n",
    "        .write.mode(\"overwrite\")\n",
    "        .parquet(s\"../data/parquetnopart/$tableName.parquet\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa565d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.foreach(x => toParquet(x, partCol(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae6e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.foreach(x => toParquetNoPart(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7ddc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CMPLNT_NUM: string (nullable = true)\n",
      " |-- CMPLNT_FR_DT: string (nullable = true)\n",
      " |-- CMPLNT_FR_TM: string (nullable = true)\n",
      " |-- CMPLNT_TO_DT: string (nullable = true)\n",
      " |-- CMPLNT_TO_TM: string (nullable = true)\n",
      " |-- RPT_DT: string (nullable = true)\n",
      " |-- KY_CD: string (nullable = true)\n",
      " |-- OFNS_DESC: string (nullable = true)\n",
      " |-- PD_CD: string (nullable = true)\n",
      " |-- PD_DESC: string (nullable = true)\n",
      " |-- CRM_ATPT_CPTD_CD: string (nullable = true)\n",
      " |-- LAW_CAT_CD: string (nullable = true)\n",
      " |-- JURIS_DESC: string (nullable = true)\n",
      " |-- BORO_NM: string (nullable = true)\n",
      " |-- ADDR_PCT_CD: string (nullable = true)\n",
      " |-- LOC_OF_OCCUR_DESC: string (nullable = true)\n",
      " |-- PREM_TYP_DESC: string (nullable = true)\n",
      " |-- PARKS_NM: string (nullable = true)\n",
      " |-- HADEVELOPT: string (nullable = true)\n",
      " |-- X_COORD_CD: string (nullable = true)\n",
      " |-- Y_COORD_CD: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Lat_Lon: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .load(s\"../data/police/NYPD_Complaint_Data_Historic.csv\").printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fb844ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres13\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m854L\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"sep\", \",\")\n",
    "        .load(s\"../data/drivers.csv\")\n",
    "        .select(\"driverId\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570ce68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
