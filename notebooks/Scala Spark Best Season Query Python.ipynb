{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/02/12 13:14:14 INFO SparkContext: Running Spark version 2.4.3\n",
      "22/02/12 13:14:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/12 13:14:14 INFO SparkContext: Submitted application: 8d5a3dd6-376a-48b2-9c20-b2c15b71ba63\n",
      "22/02/12 13:14:14 INFO SecurityManager: Changing view acls to: oscar\n",
      "22/02/12 13:14:14 INFO SecurityManager: Changing modify acls to: oscar\n",
      "22/02/12 13:14:14 INFO SecurityManager: Changing view acls groups to: \n",
      "22/02/12 13:14:14 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/02/12 13:14:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oscar); groups with view permissions: Set(); users  with modify permissions: Set(oscar); groups with modify permissions: Set()\n",
      "22/02/12 13:14:15 INFO Utils: Successfully started service 'sparkDriver' on port 43159.\n",
      "22/02/12 13:14:15 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/02/12 13:14:15 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/02/12 13:14:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/02/12 13:14:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/02/12 13:14:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-411fc712-ea23-4ab9-bb67-0b5515a64937\n",
      "22/02/12 13:14:15 INFO MemoryStore: MemoryStore started with capacity 1956.6 MB\n",
      "22/02/12 13:14:15 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/02/12 13:14:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/02/12 13:14:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://netrunner:4040\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0-sources.jar at spark://netrunner:43159/jars/jvm-repr-0.4.0-sources.jar with timestamp 1644668055369\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0.jar at spark://netrunner:43159/jars/jvm-repr-0.4.0.jar with timestamp 1644668055370\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/javaparser/javaparser-core/3.2.5/javaparser-core-3.2.5-sources.jar at spark://netrunner:43159/jars/javaparser-core-3.2.5-sources.jar with timestamp 1644668055370\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/javaparser/javaparser-core/3.2.5/javaparser-core-3.2.5.jar at spark://netrunner:43159/jars/javaparser-core-3.2.5.jar with timestamp 1644668055370\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-compiler-interface_2.12/2.3.8-36-1cce53f3/ammonite-compiler-interface_2.12-2.3.8-36-1cce53f3-sources.jar at spark://netrunner:43159/jars/ammonite-compiler-interface_2.12-2.3.8-36-1cce53f3-sources.jar with timestamp 1644668055370\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-compiler-interface_2.12/2.3.8-36-1cce53f3/ammonite-compiler-interface_2.12-2.3.8-36-1cce53f3.jar at spark://netrunner:43159/jars/ammonite-compiler-interface_2.12-2.3.8-36-1cce53f3.jar with timestamp 1644668055371\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-compiler_2.12.12/2.3.8-36-1cce53f3/ammonite-compiler_2.12.12-2.3.8-36-1cce53f3-sources.jar at spark://netrunner:43159/jars/ammonite-compiler_2.12.12-2.3.8-36-1cce53f3-sources.jar with timestamp 1644668055371\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-compiler_2.12.12/2.3.8-36-1cce53f3/ammonite-compiler_2.12.12-2.3.8-36-1cce53f3.jar at spark://netrunner:43159/jars/ammonite-compiler_2.12.12-2.3.8-36-1cce53f3.jar with timestamp 1644668055371\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.12/2.3.8-36-1cce53f3/ammonite-interp-api_2.12.12-2.3.8-36-1cce53f3-sources.jar at spark://netrunner:43159/jars/ammonite-interp-api_2.12.12-2.3.8-36-1cce53f3-sources.jar with timestamp 1644668055371\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.12/2.3.8-36-1cce53f3/ammonite-interp-api_2.12.12-2.3.8-36-1cce53f3.jar at spark://netrunner:43159/jars/ammonite-interp-api_2.12.12-2.3.8-36-1cce53f3.jar with timestamp 1644668055371\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/2.3.8-36-1cce53f3/ammonite-ops_2.12-2.3.8-36-1cce53f3-sources.jar at spark://netrunner:43159/jars/ammonite-ops_2.12-2.3.8-36-1cce53f3-sources.jar with timestamp 1644668055371\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/2.3.8-36-1cce53f3/ammonite-ops_2.12-2.3.8-36-1cce53f3.jar at spark://netrunner:43159/jars/ammonite-ops_2.12-2.3.8-36-1cce53f3.jar with timestamp 1644668055372\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.12/2.3.8-36-1cce53f3/ammonite-repl-api_2.12.12-2.3.8-36-1cce53f3-sources.jar at spark://netrunner:43159/jars/ammonite-repl-api_2.12.12-2.3.8-36-1cce53f3-sources.jar with timestamp 1644668055372\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.12/2.3.8-36-1cce53f3/ammonite-repl-api_2.12.12-2.3.8-36-1cce53f3.jar at spark://netrunner:43159/jars/ammonite-repl-api_2.12.12-2.3.8-36-1cce53f3.jar with timestamp 1644668055372\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/2.3.8-36-1cce53f3/ammonite-util_2.12-2.3.8-36-1cce53f3-sources.jar at spark://netrunner:43159/jars/ammonite-util_2.12-2.3.8-36-1cce53f3-sources.jar with timestamp 1644668055372\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/2.3.8-36-1cce53f3/ammonite-util_2.12-2.3.8-36-1cce53f3.jar at spark://netrunner:43159/jars/ammonite-util_2.12-2.3.8-36-1cce53f3.jar with timestamp 1644668055372\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.9/fansi_2.12-0.2.9-sources.jar at spark://netrunner:43159/jars/fansi_2.12-0.2.9-sources.jar with timestamp 1644668055372\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.9/fansi_2.12-0.2.9.jar at spark://netrunner:43159/jars/fansi_2.12-0.2.9.jar with timestamp 1644668055372\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fastparse_2.12/2.3.0/fastparse_2.12-2.3.0-sources.jar at spark://netrunner:43159/jars/fastparse_2.12-2.3.0-sources.jar with timestamp 1644668055373\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fastparse_2.12/2.3.0/fastparse_2.12-2.3.0.jar at spark://netrunner:43159/jars/fastparse_2.12-2.3.0.jar with timestamp 1644668055373\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.6.7/geny_2.12-0.6.7-sources.jar at spark://netrunner:43159/jars/geny_2.12-0.6.7-sources.jar with timestamp 1644668055373\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.6.7/geny_2.12-0.6.7.jar at spark://netrunner:43159/jars/geny_2.12-0.6.7.jar with timestamp 1644668055373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/mainargs_2.12/0.1.4/mainargs_2.12-0.1.4-sources.jar at spark://netrunner:43159/jars/mainargs_2.12-0.1.4-sources.jar with timestamp 1644668055373\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/mainargs_2.12/0.1.4/mainargs_2.12-0.1.4.jar at spark://netrunner:43159/jars/mainargs_2.12-0.1.4.jar with timestamp 1644668055373\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.7.1/os-lib_2.12-0.7.1-sources.jar at spark://netrunner:43159/jars/os-lib_2.12-0.7.1-sources.jar with timestamp 1644668055373\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.7.1/os-lib_2.12-0.7.1.jar at spark://netrunner:43159/jars/os-lib_2.12-0.7.1.jar with timestamp 1644668055374\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.6.0/pprint_2.12-0.6.0-sources.jar at spark://netrunner:43159/jars/pprint_2.12-0.6.0-sources.jar with timestamp 1644668055374\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.6.0/pprint_2.12-0.6.0.jar at spark://netrunner:43159/jars/pprint_2.12-0.6.0.jar with timestamp 1644668055374\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/scalaparse_2.12/2.3.0/scalaparse_2.12-2.3.0-sources.jar at spark://netrunner:43159/jars/scalaparse_2.12-2.3.0-sources.jar with timestamp 1644668055374\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/scalaparse_2.12/2.3.0/scalaparse_2.12-2.3.0.jar at spark://netrunner:43159/jars/scalaparse_2.12-2.3.0.jar with timestamp 1644668055374\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.2.3/sourcecode_2.12-0.2.3-sources.jar at spark://netrunner:43159/jars/sourcecode_2.12-0.2.3-sources.jar with timestamp 1644668055374\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.2.3/sourcecode_2.12-0.2.3.jar at spark://netrunner:43159/jars/sourcecode_2.12-0.2.3.jar with timestamp 1644668055375\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/1.0.3/interface-1.0.3-sources.jar at spark://netrunner:43159/jars/interface-1.0.3-sources.jar with timestamp 1644668055375\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/1.0.3/interface-1.0.3.jar at spark://netrunner:43159/jars/interface-1.0.3.jar with timestamp 1644668055375\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA-sources.jar at spark://netrunner:43159/jars/javassist-3.21.0-GA-sources.jar with timestamp 1644668055375\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/javassist/javassist/3.21.0-GA/javassist-3.21.0-GA.jar at spark://netrunner:43159/jars/javassist-3.21.0-GA.jar with timestamp 1644668055375\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.4.3/scala-collection-compat_2.12-2.4.3-sources.jar at spark://netrunner:43159/jars/scala-collection-compat_2.12-2.4.3-sources.jar with timestamp 1644668055376\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.4.3/scala-collection-compat_2.12-2.4.3.jar at spark://netrunner:43159/jars/scala-collection-compat_2.12-2.4.3.jar with timestamp 1644668055376\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/2.0.0-M3/scala-xml_2.12-2.0.0-M3-sources.jar at spark://netrunner:43159/jars/scala-xml_2.12-2.0.0-M3-sources.jar with timestamp 1644668055376\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/2.0.0-M3/scala-xml_2.12-2.0.0-M3.jar at spark://netrunner:43159/jars/scala-xml_2.12-2.0.0-M3.jar with timestamp 1644668055376\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.12/scala-compiler-2.12.12-sources.jar at spark://netrunner:43159/jars/scala-compiler-2.12.12-sources.jar with timestamp 1644668055377\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.12/scala-library-2.12.12-sources.jar at spark://netrunner:43159/jars/scala-library-2.12.12-sources.jar with timestamp 1644668055377\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.12/scala-reflect-2.12.12-sources.jar at spark://netrunner:43159/jars/scala-reflect-2.12.12-sources.jar with timestamp 1644668055377\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.11.2/interpreter-api_2.12-0.11.2-sources.jar at spark://netrunner:43159/jars/interpreter-api_2.12-0.11.2-sources.jar with timestamp 1644668055377\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.11.2/interpreter-api_2.12-0.11.2.jar at spark://netrunner:43159/jars/interpreter-api_2.12-0.11.2.jar with timestamp 1644668055377\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.11.2/jupyter-api_2.12-0.11.2-sources.jar at spark://netrunner:43159/jars/jupyter-api_2.12-0.11.2-sources.jar with timestamp 1644668055377\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.11.2/jupyter-api_2.12-0.11.2.jar at spark://netrunner:43159/jars/jupyter-api_2.12-0.11.2.jar with timestamp 1644668055378\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.12/0.11.2/scala-kernel-api_2.12.12-0.11.2-sources.jar at spark://netrunner:43159/jars/scala-kernel-api_2.12.12-0.11.2-sources.jar with timestamp 1644668055378\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.12/0.11.2/scala-kernel-api_2.12.12-0.11.2.jar at spark://netrunner:43159/jars/scala-kernel-api_2.12.12-0.11.2.jar with timestamp 1644668055378\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.local/share/jupyter/kernels/scala212/launcher.jar at spark://netrunner:43159/jars/launcher.jar with timestamp 1644668055378\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.13/scala-library-2.12.13.jar at spark://netrunner:43159/jars/scala-library-2.12.13.jar with timestamp 1644668055378\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/almond-spark_2.12/0.6.0/almond-spark_2.12-0.6.0.jar at spark://netrunner:43159/jars/almond-spark_2.12-0.6.0.jar with timestamp 1644668055379\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/ammonite-spark_2.12/0.5.0/ammonite-spark_2.12-0.5.0.jar at spark://netrunner:43159/jars/ammonite-spark_2.12-0.5.0.jar with timestamp 1644668055379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/alexarchambault/argonaut-shapeless_6.2_2.12/1.2.0-M11/argonaut-shapeless_6.2_2.12-1.2.0-M11.jar at spark://netrunner:43159/jars/argonaut-shapeless_6.2_2.12-1.2.0-M11.jar with timestamp 1644668055379\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.4.19.v20190610/jetty-server-9.4.19.v20190610.jar at spark://netrunner:43159/jars/jetty-server-9.4.19.v20190610.jar with timestamp 1644668055379\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/io/argonaut/argonaut_2.12/6.2.3/argonaut_2.12-6.2.3.jar at spark://netrunner:43159/jars/argonaut_2.12-6.2.3.jar with timestamp 1644668055379\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/com/chuusai/shapeless_2.12/2.3.3/shapeless_2.12-2.3.3.jar at spark://netrunner:43159/jars/shapeless_2.12-2.3.3.jar with timestamp 1644668055379\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/9.4.19.v20190610/jetty-http-9.4.19.v20190610.jar at spark://netrunner:43159/jars/jetty-http-9.4.19.v20190610.jar with timestamp 1644668055390\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.4.19.v20190610/jetty-io-9.4.19.v20190610.jar at spark://netrunner:43159/jars/jetty-io-9.4.19.v20190610.jar with timestamp 1644668055390\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/typelevel/macro-compat_2.12/1.1.1/macro-compat_2.12-1.1.1.jar at spark://netrunner:43159/jars/macro-compat_2.12-1.1.1.jar with timestamp 1644668055390\n",
      "22/02/12 13:14:15 INFO SparkContext: Added JAR file:/home/oscar/.cache/coursier/v1/https/repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.19.v20190610/jetty-util-9.4.19.v20190610.jar at spark://netrunner:43159/jars/jetty-util-9.4.19.v20190610.jar with timestamp 1644668055391\n",
      "22/02/12 13:14:15 INFO Executor: Starting executor ID driver on host localhost\n",
      "22/02/12 13:14:15 INFO Executor: Using REPL class URI: http://192.168.1.228:41707\n",
      "22/02/12 13:14:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45639.\n",
      "22/02/12 13:14:15 INFO NettyBlockTransferService: Server created on netrunner:45639\n",
      "22/02/12 13:14:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/02/12 13:14:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, netrunner, 45639, None)\n",
      "22/02/12 13:14:15 INFO BlockManagerMasterEndpoint: Registering block manager netrunner:45639 with 1956.6 MB RAM, BlockManagerId(driver, netrunner, 45639, None)\n",
      "22/02/12 13:14:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, netrunner, 45639, None)\n",
      "22/02/12 13:14:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, netrunner, 45639, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://netrunner:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3b47a6e5\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrun\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.3`\n",
    "import $ivy.`sh.almond::almond-spark:0.6.0`\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val spark = NotebookSparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.join.preferSortMergeJoin\", false)\n",
    "      .config(\"spark.sql.shuffle.partitions\", 64)\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ede4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "455d314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/12 18:05:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/12 18:05:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "      .config(\"spark.sql.shuffle.partitions\", 64)\\\n",
    "      .master(\"local[*]\")\\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2be4ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "races = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .load(\"../data/races.csv\")\\\n",
    "    .select(\"raceId\", \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c3af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonWindow = Window.partitionBy(\"year\")\n",
    "driverRaceWindow = Window.partitionBy(\"driverId\", \"raceId\")\n",
    "raceDriverLapWindow = Window.partitionBy(\"driverId\", \"raceId\").orderBy(\"lap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9365db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overtakes = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .load(\"../data/lap_times.csv\")\\\n",
    "    .withColumn(\"position\", F.col(\"position\").cast(T.IntegerType()))\\\n",
    "    .withColumn(\"lap\", F.col(\"lap\").cast(T.IntegerType()))\\\n",
    "    .join(races, \"raceId\")\\\n",
    "    .withColumn(\"positionNextLap\", F.lead(F.col(\"position\"), 1).over(raceDriverLapWindow))\\\n",
    "    .withColumn(\"positionsGainedLap\", F.when(F.col(\"positionNextLap\") < F.col(\"position\") , F.abs(F.col(\"position\") - F.col(\"positionNextLap\"))).otherwise(0))\\\n",
    "    .groupBy(\"raceId\", \"driverId\")\\\n",
    "    .agg(F.sum(F.col(\"positionsGainedLap\")).alias(\"positionsGained\"), F.first(F.col(\"year\")).alias(\"year\"))\\\n",
    "    .groupBy(\"year\")\\\n",
    "    .agg(F.sum(F.col(\"positionsGained\")).alias(\"positionsGainedSeason\"))\\\n",
    "    .withColumn(\"rankPositionsGained\", F.rank().over(Window.orderBy(F.col(\"positionsGainedSeason\").desc())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "295fc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "leadersTroughoutSeason = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .load(\"../data/driver_standings.csv\")\\\n",
    "    .join(races, \"raceId\")\\\n",
    "    .where(F.col(\"position\") == 1)\\\n",
    "    .dropDuplicates([\"driverId\", \"position\", \"year\"])\\\n",
    "    .groupBy(\"year\")\\\n",
    "    .agg(F.approx_count_distinct(F.col(\"driverId\")).alias(\"distinctLeaders\"))\\\n",
    "    .withColumn(\"rankDistinctLeaders\", F.rank().over(Window.orderBy(F.col(\"distinctLeaders\").desc())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e81c29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "winnersTroughoutSeason = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .load(\"../data/results.csv\")\\\n",
    "    .join(races, \"raceId\")\\\n",
    "    .where(F.col(\"position\") == 1)\\\n",
    "    .dropDuplicates([\"driverId\", \"position\", \"year\"])\\\n",
    "    .groupBy(\"year\")\\\n",
    "    .agg(F.approx_count_distinct(F.col(\"driverId\")).alias(\"distinctWinners\"))\\\n",
    "    .withColumn(\"rankDistinctWinners\", F.rank().over(Window.orderBy(F.col(\"distinctWinners\").desc())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fac4528e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36maverageRank\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd25$Helper$$Lambda$6499/1616635210@394c4368,\n",
       "  DoubleType,\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(\u001b[33mArrayType\u001b[39m(IntegerType, false)))\n",
       ")\n",
       "\u001b[36mres25_1\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd25$Helper$$Lambda$6499/1616635210@394c4368,\n",
       "  DoubleType,\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(\u001b[33mArrayType\u001b[39m(IntegerType, false)))\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val averageRank = udf((cols: Seq[Int]) => {cols.sum / cols.size}: Double)\n",
    "spark.udf.register(\"averageRank\", averageRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d37099ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageRank(cols):\n",
    "    return sum(cols) / len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59f46b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "averageRank = F.udf(averageRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overtakes\\\n",
    "    .join(leadersTroughoutSeason, \"year\", \"inner\")\\\n",
    "    .join(winnersTroughoutSeason, \"year\", \"inner\")\\\n",
    "    .withColumn(\"avgRank\", (F.col(\"rankDistinctWinners\") + F.col(\"rankDistinctLeaders\") + F.col(\"rankPositionsGained\") / 3))\\\n",
    "    .withColumn(\"overallRank\", F.rank().over(Window.orderBy(\"avgRank\")))\\\n",
    "    .drop(\"rankDistinctWinners\", \"rankDistinctLeaders\", \"rankPositionsGained\", \"avgRank\")\\\n",
    "    .sort(\"overallRank\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842c834",
   "metadata": {},
   "source": [
    "Usar la UDF definida arriba da resultados erróneos. Por ello se suman las columnas y se divide entre 3. Esto es menos flexible ya que si quisiera hacer la media del contenido de 6 columnas tendría que hacerlo a mano. Para solucionarlo también se puede castear la columna avgRank a Integer. Por defecto debe ser que las UDF devuelven un String en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7552edd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+---------------+---------------+-----------+\n",
      "|year|positionsGainedSeason|distinctLeaders|distinctWinners|overallRank|\n",
      "+----+---------------------+---------------+---------------+-----------+\n",
      "|2012|                 5077|              4|              8|          1|\n",
      "|2008|                 3123|              4|              7|          2|\n",
      "|2003|                 2955|              3|              8|          3|\n",
      "|1997|                 2642|              3|              6|          4|\n",
      "|2010|                 2747|              6|              5|          5|\n",
      "|1999|                 2151|              3|              6|          6|\n",
      "|2021|                 2901|              2|              6|          7|\n",
      "|2013|                 4697|              2|              5|          8|\n",
      "|2019|                 3201|              2|              5|          9|\n",
      "|2007|                 2974|              3|              4|         10|\n",
      "|2020|                 2612|              2|              5|         11|\n",
      "|2018|                 2692|              2|              5|         11|\n",
      "|2006|                 2524|              2|              5|         13|\n",
      "|2005|                 2325|              2|              5|         13|\n",
      "|2017|                 2167|              2|              5|         15|\n",
      "|2016|                 4613|              2|              4|         16|\n",
      "|2009|                 2768|              1|              6|         16|\n",
      "|2011|                 4627|              1|              5|         18|\n",
      "|2004|                 3194|              1|              5|         19|\n",
      "|2000|                 2744|              2|              4|         20|\n",
      "+----+---------------------+---------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/12 19:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/02/12 19:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "overtakes\\\n",
    "    .join(leadersTroughoutSeason, \"year\", \"inner\")\\\n",
    "    .join(winnersTroughoutSeason, \"year\", \"inner\")\\\n",
    "    .withColumn(\"avgRank\", averageRank(F.array(F.col(\"rankDistinctWinners\"), F.col(\"rankDistinctLeaders\"), F.col(\"rankPositionsGained\"))).cast(T.IntegerType()))\\\n",
    "    .withColumn(\"overallRank\", F.rank().over(Window.orderBy(\"avgRank\")))\\\n",
    "    .drop(\"rankDistinctWinners\", \"rankDistinctLeaders\", \"rankPositionsGained\", \"avgRank\")\\\n",
    "    .sort(\"overallRank\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071274c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
