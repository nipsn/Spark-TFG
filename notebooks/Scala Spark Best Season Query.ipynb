{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/05/12 09:31:10 INFO SparkContext: Running Spark version 3.2.0\n",
      "22/05/12 09:31:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/12 09:31:10 INFO ResourceUtils: ==============================================================\n",
      "22/05/12 09:31:10 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/05/12 09:31:10 INFO ResourceUtils: ==============================================================\n",
      "22/05/12 09:31:10 INFO SparkContext: Submitted application: 8faf069a-b59e-435e-882d-ea90e5681f34\n",
      "22/05/12 09:31:10 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/05/12 09:31:10 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/05/12 09:31:10 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/05/12 09:31:10 INFO SecurityManager: Changing view acls to: oscar\n",
      "22/05/12 09:31:10 INFO SecurityManager: Changing modify acls to: oscar\n",
      "22/05/12 09:31:10 INFO SecurityManager: Changing view acls groups to: \n",
      "22/05/12 09:31:10 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/05/12 09:31:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oscar); groups with view permissions: Set(); users  with modify permissions: Set(oscar); groups with modify permissions: Set()\n",
      "22/05/12 09:31:10 INFO Utils: Successfully started service 'sparkDriver' on port 35361.\n",
      "22/05/12 09:31:10 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/05/12 09:31:10 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/05/12 09:31:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/05/12 09:31:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/05/12 09:31:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/12 09:31:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2be64c44-b4ae-47af-b764-6348bdd05302\n",
      "22/05/12 09:31:10 INFO MemoryStore: MemoryStore started with capacity 4.0 GiB\n",
      "22/05/12 09:31:10 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/05/12 09:31:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/12 09:31:10 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "22/05/12 09:31:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://netrunner:4041\n",
      "22/05/12 09:31:10 INFO Executor: Starting executor ID driver on host netrunner\n",
      "22/05/12 09:31:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38105.\n",
      "22/05/12 09:31:10 INFO NettyBlockTransferService: Server created on netrunner:38105\n",
      "22/05/12 09:31:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/05/12 09:31:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, netrunner, 38105, None)\n",
      "22/05/12 09:31:11 INFO BlockManagerMasterEndpoint: Registering block manager netrunner:38105 with 4.0 GiB RAM, BlockManagerId(driver, netrunner, 38105, None)\n",
      "22/05/12 09:31:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, netrunner, 38105, None)\n",
      "22/05/12 09:31:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, netrunner, 38105, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7dfd704f\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.2.0` // Or use any other 2.x version here\n",
    "import $ivy.`sh.almond::almond-spark:0.10.1`\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "      .master(\"local[4]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2be4ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mraces\u001b[39m: \u001b[32mDataFrame\u001b[39m = [raceId: string, year: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val races = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"../data/races.csv\")\n",
    "    .select(\"raceId\", \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f844c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c3af81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mseasonWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@248716\n",
       "\u001b[36mdriverRaceWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@7c18ec24\n",
       "\u001b[36mraceDriverLapWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@69d80181"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seasonWindow = Window.partitionBy(\"year\")\n",
    "val driverRaceWindow = Window.partitionBy(\"driverId\", \"raceId\")\n",
    "val raceDriverLapWindow = Window.partitionBy(\"driverId\", \"raceId\").orderBy(\"lap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9365db4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36movertakes\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, positionsGainedSeason: bigint ... 1 more field]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overtakes = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"../data/lap_times.csv\")\n",
    "    .withColumn(\"position\", col(\"position\").cast(IntegerType)) \n",
    "    .withColumn(\"lap\", col(\"lap\").cast(IntegerType)) \n",
    "    .join(races, \"raceId\")\n",
    "    .withColumn(\"positionNextLap\", lead(col(\"position\"), 1).over(raceDriverLapWindow))\n",
    "    .withColumn(\"positionsGainedLap\", when(col(\"positionNextLap\") < col(\"position\") , abs(col(\"position\") - col(\"positionNextLap\"))).otherwise(0))\n",
    "    .groupBy(\"raceId\", \"driverId\")\n",
    "    .agg(sum(col(\"positionsGainedLap\")).alias(\"positionsGained\"), first(col(\"year\")).alias(\"year\"))\n",
    "    .groupBy(\"year\")\n",
    "    .agg(sum(col(\"positionsGained\")).alias(\"positionsGainedSeason\"))\n",
    "    .withColumn(\"rankPositionsGained\", rank().over(Window.orderBy(col(\"positionsGainedSeason\").desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295fc145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mleadersTroughoutSeason\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, distinctLeaders: bigint ... 1 more field]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val leadersTroughoutSeason = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"../data/driver_standings.csv\")\n",
    "    .join(races, \"raceId\")\n",
    "    .where(col(\"position\") === 1)\n",
    "    .dropDuplicates(\"driverId\", \"position\", \"year\")\n",
    "    .groupBy(\"year\")\n",
    "    .agg(approx_count_distinct(col(\"driverId\")).alias(\"distinctLeaders\"))\n",
    "    .withColumn(\"rankDistinctLeaders\", rank().over(Window.orderBy(col(\"distinctLeaders\").desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81c29e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mwinnersTroughoutSeason\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, distinctWinners: bigint ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val winnersTroughoutSeason = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"../data/results.csv\")\n",
    "    .join(races, \"raceId\")\n",
    "    .where(col(\"position\") === 1)\n",
    "    .dropDuplicates(\"driverId\", \"position\", \"year\")\n",
    "    .groupBy(\"year\")\n",
    "    .agg(approx_count_distinct(col(\"driverId\")).alias(\"distinctWinners\"))\n",
    "    .sort(col(\"distinctWinners\").desc)\n",
    "    .withColumn(\"rankDistinctWinners\", rank().over(Window.orderBy(col(\"distinctWinners\").desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d20ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36maverageRank\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd7$Helper$$Lambda$5373/1093631140@6e017788,\n",
       "  DoubleType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mNewInstance\u001b[39m(\n",
       "          class org.apache.spark.sql.catalyst.util.GenericArrayData,\n",
       "          \u001b[33mList\u001b[39m(\n",
       "            \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(interface scala.collection.Seq), true)\n",
       "          ),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          \u001b[33mArrayType\u001b[39m(IntegerType, false),\n",
       "          \u001b[32mNone\u001b[39m\n",
       "        ),\n",
       "        \u001b[33mUnresolvedMapObjects\u001b[39m(\n",
       "          org.apache.spark.sql.catalyst.ScalaReflection$$$Lambda$5380/1908438313@f4a956d,\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mArrayType\u001b[39m(IntegerType, false)),\n",
       "          \u001b[33mSome\u001b[39m(interface scala.collection.Seq)\n",
       "        ),\n",
       "        scala.collection.Seq\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, DoubleType, false),\n",
       "      \u001b[33mAssertNotNull\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, DoubleType),\n",
       "          DoubleType,\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "      ),\n",
       "      Double\n",
       "    )\n",
       "...\n",
       "\u001b[36mres7_1\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd7$Helper$$Lambda$5373/1093631140@6e017788,\n",
       "  DoubleType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mNewInstance\u001b[39m(\n",
       "          class org.apache.spark.sql.catalyst.util.GenericArrayData,\n",
       "          \u001b[33mList\u001b[39m(\n",
       "            \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(interface scala.collection.Seq), true)\n",
       "          ),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          \u001b[33mArrayType\u001b[39m(IntegerType, false),\n",
       "          \u001b[32mNone\u001b[39m\n",
       "        ),\n",
       "        \u001b[33mUnresolvedMapObjects\u001b[39m(\n",
       "          org.apache.spark.sql.catalyst.ScalaReflection$$$Lambda$5380/1908438313@f4a956d,\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mArrayType\u001b[39m(IntegerType, false)),\n",
       "          \u001b[33mSome\u001b[39m(interface scala.collection.Seq)\n",
       "        ),\n",
       "        scala.collection.Seq\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, DoubleType, false),\n",
       "      \u001b[33mAssertNotNull\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, DoubleType),\n",
       "          DoubleType,\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "      ),\n",
       "      Double\n",
       "    )\n",
       "..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val averageRank = udf((cols: Seq[Int]) => {cols.sum / cols.size}: Double)\n",
    "spark.udf.register(\"averageRank\", averageRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7552edd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mresults\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, positionsGainedSeason: bigint ... 7 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = overtakes\n",
    "    .join(leadersTroughoutSeason, Seq(\"year\"), \"inner\")\n",
    "    .join(winnersTroughoutSeason, Seq(\"year\"), \"inner\")\n",
    "    .withColumn(\"avgRank\", averageRank(array(col(\"rankDistinctWinners\"), col(\"rankDistinctLeaders\"), col(\"rankPositionsGained\"))))\n",
    "    .withColumn(\"overallRank\", rank().over(Window.orderBy(\"avgRank\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec1eac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+---------------------+---------------+---------------+\n",
      "|year|overallRank|positionsGainedSeason|distinctLeaders|distinctWinners|\n",
      "+----+-----------+---------------------+---------------+---------------+\n",
      "|2012|1          |5077                 |4              |8              |\n",
      "|2008|2          |3123                 |4              |7              |\n",
      "|2003|3          |2955                 |3              |8              |\n",
      "|1997|4          |2642                 |3              |6              |\n",
      "|2010|5          |2747                 |6              |5              |\n",
      "|1999|6          |2151                 |3              |6              |\n",
      "|2021|7          |2901                 |2              |6              |\n",
      "|2013|8          |4697                 |2              |5              |\n",
      "|2019|9          |3201                 |2              |5              |\n",
      "|2007|10         |2974                 |3              |4              |\n",
      "|2018|11         |2692                 |2              |5              |\n",
      "|2020|11         |2612                 |2              |5              |\n",
      "|2006|13         |2524                 |2              |5              |\n",
      "|2005|13         |2325                 |2              |5              |\n",
      "|2017|15         |2167                 |2              |5              |\n",
      "|2016|16         |4613                 |2              |4              |\n",
      "|2009|16         |2768                 |1              |6              |\n",
      "|2011|18         |4627                 |1              |5              |\n",
      "|2004|19         |3194                 |1              |5              |\n",
      "|2000|20         |2744                 |2              |4              |\n",
      "|2014|21         |3873                 |2              |3              |\n",
      "|2001|22         |2256                 |1              |5              |\n",
      "|2002|23         |2294                 |1              |4              |\n",
      "|2015|24         |2747                 |1              |3              |\n",
      "|1996|24         |1946                 |1              |4              |\n",
      "|1998|24         |1827                 |1              |4              |\n",
      "+----+-----------+---------------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(\"year\", \"overallRank\", \"positionsGainedSeason\", \"distinctLeaders\", \"distinctWinners\").show(70, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc11a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrun\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36maverage\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeTest\u001b[39m\n",
       "\u001b[36mres\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2390L\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run[A](code: => A): Long = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    System.currentTimeMillis() - start\n",
    "}\n",
    "\n",
    "def average(list: List[Long]):Long = list.sum / list.size.toLong\n",
    "\n",
    "def timeTest(f: => Long): Long = {\n",
    "    val list = (1 to 1).map(_ => f).toList\n",
    "    average(list)\n",
    "}\n",
    "\n",
    "val res = timeTest(run(results.collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c9384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c8a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a47542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49da2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
