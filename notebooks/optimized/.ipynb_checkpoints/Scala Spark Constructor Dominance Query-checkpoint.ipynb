{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90d563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/04/18 10:48:20 INFO SparkContext: Running Spark version 3.2.0\n",
      "22/04/18 10:48:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/18 10:48:20 INFO ResourceUtils: ==============================================================\n",
      "22/04/18 10:48:20 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/04/18 10:48:20 INFO ResourceUtils: ==============================================================\n",
      "22/04/18 10:48:20 INFO SparkContext: Submitted application: dfffad4c-6db8-43fa-bb29-77816f2dcc04\n",
      "22/04/18 10:48:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/04/18 10:48:20 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/04/18 10:48:20 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/04/18 10:48:20 INFO SecurityManager: Changing view acls to: oscar\n",
      "22/04/18 10:48:20 INFO SecurityManager: Changing modify acls to: oscar\n",
      "22/04/18 10:48:20 INFO SecurityManager: Changing view acls groups to: \n",
      "22/04/18 10:48:20 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/04/18 10:48:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oscar); groups with view permissions: Set(); users  with modify permissions: Set(oscar); groups with modify permissions: Set()\n",
      "22/04/18 10:48:21 INFO Utils: Successfully started service 'sparkDriver' on port 32801.\n",
      "22/04/18 10:48:21 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/04/18 10:48:21 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/04/18 10:48:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/04/18 10:48:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/04/18 10:48:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/18 10:48:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f279def9-30d9-430c-bebe-227e60cff51a\n",
      "22/04/18 10:48:21 INFO MemoryStore: MemoryStore started with capacity 1956.6 MiB\n",
      "22/04/18 10:48:21 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/04/18 10:48:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/04/18 10:48:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://netrunner:4040\n",
      "22/04/18 10:48:21 INFO Executor: Starting executor ID driver on host netrunner\n",
      "22/04/18 10:48:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40237.\n",
      "22/04/18 10:48:21 INFO NettyBlockTransferService: Server created on netrunner:40237\n",
      "22/04/18 10:48:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/04/18 10:48:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, netrunner, 40237, None)\n",
      "22/04/18 10:48:21 INFO BlockManagerMasterEndpoint: Registering block manager netrunner:40237 with 1956.6 MiB RAM, BlockManagerId(driver, netrunner, 40237, None)\n",
      "22/04/18 10:48:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, netrunner, 40237, None)\n",
      "22/04/18 10:48:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, netrunner, 40237, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@9d0e7e8\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.2.0` // Or use any other 2.x version here\n",
    "import $ivy.`sh.almond::almond-spark:0.10.1`\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[4]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8407a7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35186ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlastRace\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@4b11edc2\n",
       "\u001b[36mlastRaces\u001b[39m: \u001b[32mDataFrame\u001b[39m = [raceId: string, year: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lastRace = Window.partitionBy(\"year\")\n",
    "\n",
    "val lastRaces = spark.read.parquet(\"../../data/parquet/races.parquet\")\n",
    "    .where(col(\"year\") >= 1990 && col(\"year\") <= 1999)\n",
    "    .withColumn(\"round\", col(\"round\").cast(IntegerType))\n",
    "    .withColumn(\"max\", max(col(\"round\")).over(lastRace))\n",
    "    .where(col(\"round\") === col(\"max\"))\n",
    "    .select(\"raceId\", \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbf9253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mconstructors\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@5edcb4f\n",
       "\u001b[36mconstructorWinners\u001b[39m: \u001b[32mDataFrame\u001b[39m = [constructorId: string, year: int ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val constructors = Window.partitionBy(\"constructorId\")\n",
    "\n",
    "val constructorWinners = spark.read.parquet(\"../../data/parquet/constructor_standings.parquet\")\n",
    "    .join(lastRaces, Seq(\"raceId\"), \"right\")\n",
    "    .where(col(\"position\") === 1)\n",
    "    .select(\"constructorId\", \"wins\", \"year\")\n",
    "    .withColumn(\"totalChampWins\", count(col(\"constructorId\")).over(constructors))\n",
    "    .withColumn(\"totalRaceWins\", sum(col(\"wins\")).over(constructors).cast(IntegerType))\n",
    "    .drop(\"wins\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299ab61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mconstructors\u001b[39m: \u001b[32mDataFrame\u001b[39m = [constructorId: string, name: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val constructors = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .load(\"../data/constructors.csv\")\n",
    "    .select(\"constructorId\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e113c5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmostDominantConstructor\u001b[39m: \u001b[32mDataFrame\u001b[39m = [totalChampWins: bigint, totalRaceWins: int ... 1 more field]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mostDominantConstructor = constructorWinners\n",
    "    .drop(\"year\")\n",
    "    .dropDuplicates(\"constructorId\")\n",
    "    .orderBy(col(\"totalChampWins\").desc, col(\"totalRaceWins\").desc)\n",
    "    .join(constructors, \"constructorId\")\n",
    "    .drop(\"constructorId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca913bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrun\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36maverage\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeTest\u001b[39m\n",
       "\u001b[36mres\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1378L\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run[A](code: => A): Long = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    System.currentTimeMillis() - start\n",
    "}\n",
    "\n",
    "def average(list: List[Long]):Long = list.sum / list.size.toLong\n",
    "\n",
    "def timeTest(f: => Long): Long = {\n",
    "    val list = (1 to 1).map(_ => f).toList\n",
    "    average(list)\n",
    "}\n",
    "\n",
    "val res = timeTest(run(mostDominantConstructor.collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d267a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
