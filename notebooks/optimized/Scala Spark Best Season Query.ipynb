{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/05/04 21:26:44 INFO SparkContext: Running Spark version 3.2.0\n",
      "22/05/04 21:26:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/04 21:26:44 INFO ResourceUtils: ==============================================================\n",
      "22/05/04 21:26:44 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/05/04 21:26:44 INFO ResourceUtils: ==============================================================\n",
      "22/05/04 21:26:44 INFO SparkContext: Submitted application: 10f723c3-27d7-4e22-9425-1492bfb2026f\n",
      "22/05/04 21:26:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/05/04 21:26:44 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/05/04 21:26:44 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/05/04 21:26:44 INFO SecurityManager: Changing view acls to: oscar\n",
      "22/05/04 21:26:44 INFO SecurityManager: Changing modify acls to: oscar\n",
      "22/05/04 21:26:44 INFO SecurityManager: Changing view acls groups to: \n",
      "22/05/04 21:26:44 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/05/04 21:26:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oscar); groups with view permissions: Set(); users  with modify permissions: Set(oscar); groups with modify permissions: Set()\n",
      "22/05/04 21:26:44 INFO Utils: Successfully started service 'sparkDriver' on port 46101.\n",
      "22/05/04 21:26:44 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/05/04 21:26:44 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/05/04 21:26:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/05/04 21:26:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/05/04 21:26:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/04 21:26:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-96e3a552-079d-4f41-b19e-8f2fa366506c\n",
      "22/05/04 21:26:45 INFO MemoryStore: MemoryStore started with capacity 4.0 GiB\n",
      "22/05/04 21:26:45 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/05/04 21:26:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/04 21:26:45 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "22/05/04 21:26:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://netrunner:4041\n",
      "22/05/04 21:26:45 INFO Executor: Starting executor ID driver on host netrunner\n",
      "22/05/04 21:26:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40415.\n",
      "22/05/04 21:26:45 INFO NettyBlockTransferService: Server created on netrunner:40415\n",
      "22/05/04 21:26:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/05/04 21:26:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, netrunner, 40415, None)\n",
      "22/05/04 21:26:45 INFO BlockManagerMasterEndpoint: Registering block manager netrunner:40415 with 4.0 GiB RAM, BlockManagerId(driver, netrunner, 40415, None)\n",
      "22/05/04 21:26:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, netrunner, 40415, None)\n",
      "22/05/04 21:26:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, netrunner, 40415, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2a3abf7a\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.2.0` // Or use any other 2.x version here\n",
    "import $ivy.`sh.almond::almond-spark:0.10.1`\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "      .master(\"local[4]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2be4ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mraces\u001b[39m: \u001b[32mDataFrame\u001b[39m = [raceId: string, year: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val races = spark.read.parquet(\"../../data/parquet/races.parquet\")\n",
    "    .select(\"raceId\", \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f844c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c3af81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mseasonWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@84f7fae\n",
       "\u001b[36mdriverRaceWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@27af238e\n",
       "\u001b[36mraceDriverLapWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@7fd5aad0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seasonWindow = Window.partitionBy(\"year\")\n",
    "val driverRaceWindow = Window.partitionBy(\"driverId\", \"raceId\")\n",
    "val raceDriverLapWindow = Window.partitionBy(\"driverId\", \"raceId\").orderBy(\"lap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9365db4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36movertakes\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, positionsGainedSeason: bigint ... 1 more field]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val overtakes = spark.read.parquet(\"../../data/parquet/lap_times.parquet\")\n",
    "    .withColumn(\"position\", col(\"position\").cast(IntegerType)) \n",
    "    .withColumn(\"lap\", col(\"lap\").cast(IntegerType)) \n",
    "    .join(races, \"raceId\")\n",
    "    .withColumn(\"positionNextLap\", lead(col(\"position\"), 1).over(raceDriverLapWindow))\n",
    "    .withColumn(\"positionsGainedLap\", when(col(\"positionNextLap\") < col(\"position\") , abs(col(\"position\") - col(\"positionNextLap\"))).otherwise(0))\n",
    "    .groupBy(\"raceId\", \"driverId\")\n",
    "    .agg(sum(col(\"positionsGainedLap\")).alias(\"positionsGained\"), first(col(\"year\")).alias(\"year\"))\n",
    "    .groupBy(\"year\")\n",
    "    .agg(sum(col(\"positionsGained\")).alias(\"positionsGainedSeason\"))\n",
    "    .withColumn(\"rankPositionsGained\", rank().over(Window.orderBy(col(\"positionsGainedSeason\").desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295fc145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mleadersTroughoutSeason\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, distinctLeaders: bigint ... 1 more field]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val leadersTroughoutSeason = spark.read.parquet(\"../../data/parquet/driver_standings.parquet\")\n",
    "    .join(races, \"raceId\")\n",
    "    .where(col(\"position\") === 1)\n",
    "    .dropDuplicates(\"driverId\", \"position\", \"year\")\n",
    "    .groupBy(\"year\")\n",
    "    .agg(approx_count_distinct(col(\"driverId\")).alias(\"distinctLeaders\"))\n",
    "    .withColumn(\"rankDistinctLeaders\", rank().over(Window.orderBy(col(\"distinctLeaders\").desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81c29e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mwinnersTroughoutSeason\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, distinctWinners: bigint ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val winnersTroughoutSeason = spark.read.parquet(\"../../data/parquet/results.parquet\")\n",
    "    .join(races, \"raceId\")\n",
    "    .where(col(\"position\") === 1)\n",
    "    .dropDuplicates(\"driverId\", \"position\", \"year\")\n",
    "    .groupBy(\"year\")\n",
    "    .agg(approx_count_distinct(col(\"driverId\")).alias(\"distinctWinners\"))\n",
    "    .sort(col(\"distinctWinners\").desc)\n",
    "    .withColumn(\"rankDistinctWinners\", rank().over(Window.orderBy(col(\"distinctWinners\").desc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d20ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36maverageRank\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd7$Helper$$Lambda$4907/2101839784@584bc373,\n",
       "  DoubleType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mNewInstance\u001b[39m(\n",
       "          class org.apache.spark.sql.catalyst.util.GenericArrayData,\n",
       "          \u001b[33mList\u001b[39m(\n",
       "            \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(interface scala.collection.Seq), true)\n",
       "          ),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          \u001b[33mArrayType\u001b[39m(IntegerType, false),\n",
       "          \u001b[32mNone\u001b[39m\n",
       "        ),\n",
       "        \u001b[33mUnresolvedMapObjects\u001b[39m(\n",
       "          org.apache.spark.sql.catalyst.ScalaReflection$$$Lambda$4920/1605261429@6e34156f,\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mArrayType\u001b[39m(IntegerType, false)),\n",
       "          \u001b[33mSome\u001b[39m(interface scala.collection.Seq)\n",
       "        ),\n",
       "        scala.collection.Seq\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, DoubleType, false),\n",
       "      \u001b[33mAssertNotNull\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, DoubleType),\n",
       "          DoubleType,\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "      ),\n",
       "      Double\n",
       "    )\n",
       "...\n",
       "\u001b[36mres7_1\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd7$Helper$$Lambda$4907/2101839784@584bc373,\n",
       "  DoubleType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mNewInstance\u001b[39m(\n",
       "          class org.apache.spark.sql.catalyst.util.GenericArrayData,\n",
       "          \u001b[33mList\u001b[39m(\n",
       "            \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(interface scala.collection.Seq), true)\n",
       "          ),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          \u001b[33mArrayType\u001b[39m(IntegerType, false),\n",
       "          \u001b[32mNone\u001b[39m\n",
       "        ),\n",
       "        \u001b[33mUnresolvedMapObjects\u001b[39m(\n",
       "          org.apache.spark.sql.catalyst.ScalaReflection$$$Lambda$4920/1605261429@6e34156f,\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mArrayType\u001b[39m(IntegerType, false)),\n",
       "          \u001b[33mSome\u001b[39m(interface scala.collection.Seq)\n",
       "        ),\n",
       "        scala.collection.Seq\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, DoubleType, false),\n",
       "      \u001b[33mAssertNotNull\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, DoubleType),\n",
       "          DoubleType,\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "      ),\n",
       "      Double\n",
       "    )\n",
       "..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val averageRank = udf((cols: Seq[Int]) => {cols.sum / cols.size}: Double)\n",
    "spark.udf.register(\"averageRank\", averageRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7552edd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mresults\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, positionsGainedSeason: bigint ... 7 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = overtakes\n",
    "    .join(leadersTroughoutSeason, Seq(\"year\"), \"inner\")\n",
    "    .join(winnersTroughoutSeason, Seq(\"year\"), \"inner\")\n",
    "    .withColumn(\"avgRank\", averageRank(array(col(\"rankDistinctWinners\"), col(\"rankDistinctLeaders\"), col(\"rankPositionsGained\"))))\n",
    "    .withColumn(\"overallRank\", rank().over(Window.orderBy(\"avgRank\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc11a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrun\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36maverage\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeTest\u001b[39m\n",
       "\u001b[36mres\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m3228L\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run[A](code: => A): Long = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    System.currentTimeMillis() - start\n",
    "}\n",
    "\n",
    "def average(list: List[Long]):Long = list.sum / list.size.toLong\n",
    "\n",
    "def timeTest(f: => Long): Long = {\n",
    "    val list = (1 to 1).map(_ => f).toList\n",
    "    average(list)\n",
    "}\n",
    "\n",
    "val res = timeTest(run(results.collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c9384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c8a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a47542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49da2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
