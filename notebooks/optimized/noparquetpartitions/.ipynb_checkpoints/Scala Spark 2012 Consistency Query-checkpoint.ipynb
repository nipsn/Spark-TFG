{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/04/18 11:15:35 INFO SparkContext: Running Spark version 3.2.0\n",
      "22/04/18 11:15:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/18 11:15:35 INFO ResourceUtils: ==============================================================\n",
      "22/04/18 11:15:35 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/04/18 11:15:35 INFO ResourceUtils: ==============================================================\n",
      "22/04/18 11:15:35 INFO SparkContext: Submitted application: ff0b8266-5e61-4056-b07e-2f268469ed41\n",
      "22/04/18 11:15:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/04/18 11:15:35 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/04/18 11:15:35 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/04/18 11:15:35 INFO SecurityManager: Changing view acls to: oscar\n",
      "22/04/18 11:15:35 INFO SecurityManager: Changing modify acls to: oscar\n",
      "22/04/18 11:15:35 INFO SecurityManager: Changing view acls groups to: \n",
      "22/04/18 11:15:35 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/04/18 11:15:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oscar); groups with view permissions: Set(); users  with modify permissions: Set(oscar); groups with modify permissions: Set()\n",
      "22/04/18 11:15:35 INFO Utils: Successfully started service 'sparkDriver' on port 36617.\n",
      "22/04/18 11:15:35 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/04/18 11:15:35 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/04/18 11:15:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/04/18 11:15:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/04/18 11:15:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/18 11:15:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6798ed44-9662-4373-a18a-8df338d2d907\n",
      "22/04/18 11:15:35 INFO MemoryStore: MemoryStore started with capacity 1956.6 MiB\n",
      "22/04/18 11:15:35 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/04/18 11:15:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/04/18 11:15:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://netrunner:4040\n",
      "22/04/18 11:15:36 INFO Executor: Starting executor ID driver on host netrunner\n",
      "22/04/18 11:15:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42715.\n",
      "22/04/18 11:15:36 INFO NettyBlockTransferService: Server created on netrunner:42715\n",
      "22/04/18 11:15:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/04/18 11:15:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, netrunner, 42715, None)\n",
      "22/04/18 11:15:36 INFO BlockManagerMasterEndpoint: Registering block manager netrunner:42715 with 1956.6 MiB RAM, BlockManagerId(driver, netrunner, 42715, None)\n",
      "22/04/18 11:15:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, netrunner, 42715, None)\n",
      "22/04/18 11:15:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, netrunner, 42715, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@411cee58\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.2.0` // Or use any other 2.x version here\n",
    "import $ivy.`sh.almond::almond-spark:0.10.1`\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[4]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5313ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35186ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: Path does not exist: file:/home/oscar/TFG/Spark-TFG/notebooks/data/parquetnopart/races.parquet\u001b[39m\n  org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(\u001b[32mQueryCompilationErrors.scala\u001b[39m:\u001b[32m982\u001b[39m)\n  org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m780\u001b[39m)\n  org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m777\u001b[39m)\n  org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(\u001b[32mThreadUtils.scala\u001b[39m:\u001b[32m372\u001b[39m)\n  scala.concurrent.Future$.$anonfun$apply$1(\u001b[32mFuture.scala\u001b[39m:\u001b[32m659\u001b[39m)\n  scala.util.Success.$anonfun$map$1(\u001b[32mTry.scala\u001b[39m:\u001b[32m255\u001b[39m)\n  scala.util.Success.map(\u001b[32mTry.scala\u001b[39m:\u001b[32m213\u001b[39m)\n  scala.concurrent.Future.$anonfun$map$1(\u001b[32mFuture.scala\u001b[39m:\u001b[32m292\u001b[39m)\n  scala.concurrent.impl.Promise.liftedTree1$1(\u001b[32mPromise.scala\u001b[39m:\u001b[32m33\u001b[39m)\n  scala.concurrent.impl.Promise.$anonfun$transform$1(\u001b[32mPromise.scala\u001b[39m:\u001b[32m33\u001b[39m)\n  scala.concurrent.impl.CallbackRunnable.run(\u001b[32mPromise.scala\u001b[39m:\u001b[32m64\u001b[39m)\n  java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(\u001b[32mForkJoinTask.java\u001b[39m:\u001b[32m1402\u001b[39m)\n  java.util.concurrent.ForkJoinTask.doExec(\u001b[32mForkJoinTask.java\u001b[39m:\u001b[32m289\u001b[39m)\n  java.util.concurrent.ForkJoinPool$WorkQueue.runTask(\u001b[32mForkJoinPool.java\u001b[39m:\u001b[32m1056\u001b[39m)\n  java.util.concurrent.ForkJoinPool.runWorker(\u001b[32mForkJoinPool.java\u001b[39m:\u001b[32m1692\u001b[39m)\n  java.util.concurrent.ForkJoinWorkerThread.run(\u001b[32mForkJoinWorkerThread.java\u001b[39m:\u001b[32m175\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val races = spark.read.parquet(\"../../data/parquetnopart/races.parquet\")\n",
    "    .where(col(\"year\") === 2012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a6d320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlapTimeToMs\u001b[39m: \u001b[32mString\u001b[39m => \u001b[32mLong\u001b[39m = ammonite.$sess.cmd3$Helper$$Lambda$4940/855700990@65750d12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lapTimeToMs = (time: String) => {\n",
    "    val regex = \"\"\"([0-9]|[0-9][0-9]):([0-9][0-9])\\.([0-9][0-9][0-9])\"\"\".r\n",
    "    time match {\n",
    "        case regex(min,sec,ms) => min.toInt * 60 * 1000 + sec.toInt * 1000 + ms.toInt\n",
    "        case \"\\\\N\" => 180000\n",
    "    }\n",
    "    \n",
    "}: Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51dae689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlapTimeToMsUDF\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd3$Helper$$Lambda$4940/855700990@65750d12,\n",
       "  LongType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mStaticInvoke\u001b[39m(\n",
       "          class org.apache.spark.unsafe.types.UTF8String,\n",
       "          StringType,\n",
       "          \u001b[32m\"fromString\"\u001b[39m,\n",
       "          \u001b[33mList\u001b[39m(\u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(class java.lang.String), true)),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          false\n",
       "        ),\n",
       "        \u001b[33mInvoke\u001b[39m(\n",
       "          \u001b[33mUpCast\u001b[39m(\n",
       "            \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, StringType),\n",
       "            StringType,\n",
       "            \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"java.lang.String\\\"\"\u001b[39m)\n",
       "          ),\n",
       "          \u001b[32m\"toString\"\u001b[39m,\n",
       "          \u001b[33mObjectType\u001b[39m(class java.lang.String),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          false\n",
       "        ),\n",
       "        java.lang.String\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, LongType, false),\n",
       "      \u001b[33mAssertNotNull\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, LongType),\n",
       "          LongType,\n",
       "...\n",
       "\u001b[36mres4_1\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd3$Helper$$Lambda$4940/855700990@65750d12,\n",
       "  LongType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mStaticInvoke\u001b[39m(\n",
       "          class org.apache.spark.unsafe.types.UTF8String,\n",
       "          StringType,\n",
       "          \u001b[32m\"fromString\"\u001b[39m,\n",
       "          \u001b[33mList\u001b[39m(\u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(class java.lang.String), true)),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          false\n",
       "        ),\n",
       "        \u001b[33mInvoke\u001b[39m(\n",
       "          \u001b[33mUpCast\u001b[39m(\n",
       "            \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, StringType),\n",
       "            StringType,\n",
       "            \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"java.lang.String\\\"\"\u001b[39m)\n",
       "          ),\n",
       "          \u001b[32m\"toString\"\u001b[39m,\n",
       "          \u001b[33mObjectType\u001b[39m(class java.lang.String),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          false\n",
       "        ),\n",
       "        java.lang.String\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, LongType, false),\n",
       "      \u001b[33mAssertNotNull\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, LongType),\n",
       "          LongType,\n",
       "..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lapTimeToMsUDF = udf(lapTimeToMs)\n",
    "spark.udf.register(\"lapTimeToMs\", lapTimeToMsUDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6584db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmsToLapTime\u001b[39m: \u001b[32mLong\u001b[39m => \u001b[32mString\u001b[39m = ammonite.$sess.cmd5$Helper$$Lambda$5092/1855684642@3e24a97"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val msToLapTime = (time: Long) => {\n",
    "    val mins = time / 60000\n",
    "    val secs = (time - mins*60000)/1000\n",
    "    val ms = time - mins*60000 - secs*1000\n",
    "    \n",
    "    val formattedSecs = if((secs / 10).toInt == 0) \"0\" + secs else secs\n",
    "    // if ms = 00x -> \"0\"+\"0\"+x . if ms = 0xx -> \"0\"+ms\n",
    "    val formattedMs = if((ms / 100).toInt == 0) \"0\" + (if((ms / 10).toInt == 0) \"0\" + ms else ms) else ms\n",
    "    mins + \":\" + formattedSecs + \".\" + formattedMs    \n",
    "}: String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00edad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"1:30.234\"\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msToLapTime(lapTimeToMs(\"1:30.234\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d7e0950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmsToLapTimeUDF\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd5$Helper$$Lambda$5092/1855684642@3e24a97,\n",
       "  StringType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, LongType, false),\n",
       "        \u001b[33mAssertNotNull\u001b[39m(\n",
       "          \u001b[33mUpCast\u001b[39m(\n",
       "            \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, LongType),\n",
       "            LongType,\n",
       "            \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Long\\\"\"\u001b[39m)\n",
       "          ),\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Long\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        Long\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mStaticInvoke\u001b[39m(\n",
       "        class org.apache.spark.unsafe.types.UTF8String,\n",
       "        StringType,\n",
       "        \u001b[32m\"fromString\"\u001b[39m,\n",
       "        \u001b[33mList\u001b[39m(\u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(class java.lang.String), true)),\n",
       "        \u001b[33mList\u001b[39m(),\n",
       "        true,\n",
       "        false\n",
       "      ),\n",
       "      \u001b[33mInvoke\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, StringType),\n",
       "          StringType,\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"java.lang.String\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        \u001b[32m\"toString\"\u001b[39m,\n",
       "        \u001b[33mObjectType\u001b[39m(class java.lang.String),\n",
       "        \u001b[33mList\u001b[39m(),\n",
       "...\n",
       "\u001b[36mres7_1\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd5$Helper$$Lambda$5092/1855684642@3e24a97,\n",
       "  StringType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, LongType, false),\n",
       "        \u001b[33mAssertNotNull\u001b[39m(\n",
       "          \u001b[33mUpCast\u001b[39m(\n",
       "            \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, LongType),\n",
       "            LongType,\n",
       "            \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Long\\\"\"\u001b[39m)\n",
       "          ),\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Long\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        Long\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mStaticInvoke\u001b[39m(\n",
       "        class org.apache.spark.unsafe.types.UTF8String,\n",
       "        StringType,\n",
       "        \u001b[32m\"fromString\"\u001b[39m,\n",
       "        \u001b[33mList\u001b[39m(\u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(class java.lang.String), true)),\n",
       "        \u001b[33mList\u001b[39m(),\n",
       "        true,\n",
       "        false\n",
       "      ),\n",
       "      \u001b[33mInvoke\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, StringType),\n",
       "          StringType,\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"java.lang.String\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        \u001b[32m\"toString\"\u001b[39m,\n",
       "        \u001b[33mObjectType\u001b[39m(class java.lang.String),\n",
       "        \u001b[33mList\u001b[39m(),\n",
       "..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val msToLapTimeUDF = udf(msToLapTime)\n",
    "spark.udf.register(\"msToLapTime\", msToLapTimeUDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf82a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriverWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@28ac7217\n",
       "\u001b[36mavg_lap_times\u001b[39m: \u001b[32mDataFrame\u001b[39m = [driverId: string, avgMs: double]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverWindow = Window.partitionBy(\"driverId\")\n",
    "\n",
    "val avg_lap_times = spark.read.parquet(\"../../data/parquet/lap_times.parquet\")\n",
    "    .withColumnRenamed(\"time\", \"lapTime\")\n",
    "    .join(races, Seq(\"raceId\"), \"right\")\n",
    "    .withColumn(\"milliseconds\", col(\"milliseconds\").cast(IntegerType))\n",
    "    .withColumn(\"avgMs\", avg(col(\"milliseconds\")).over(driverWindow))\n",
    "    .dropDuplicates(\"driverId\")\n",
    "    .select(\"driverId\", \"avgMs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e05018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdrivers\u001b[39m: \u001b[32mDataFrame\u001b[39m = [driverId: string, driverRef: string ... 7 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val drivers = spark.read.parquet(\"../../data/parquet/drivers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3523de8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mseasonWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@386aed02\n",
       "\u001b[36mlapCount\u001b[39m: \u001b[32mDataFrame\u001b[39m = [raceId: string, driverId: string ... 12 more fields]\n",
       "\u001b[36mdistinctDrivers\u001b[39m: \u001b[32mBigInt\u001b[39m = 25\n",
       "\u001b[36mallLaps\u001b[39m: \u001b[32mBigInt\u001b[39m = 25343\n",
       "\u001b[36mavgLapsThisPeriod\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m1013\u001b[39m\n",
       "\u001b[36mexperiencedDrivers\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"1\"\u001b[39m,\n",
       "  \u001b[32m\"10\"\u001b[39m,\n",
       "  \u001b[32m\"13\"\u001b[39m,\n",
       "  \u001b[32m\"155\"\u001b[39m,\n",
       "  \u001b[32m\"17\"\u001b[39m,\n",
       "  \u001b[32m\"18\"\u001b[39m,\n",
       "  \u001b[32m\"20\"\u001b[39m,\n",
       "  \u001b[32m\"3\"\u001b[39m,\n",
       "  \u001b[32m\"4\"\u001b[39m,\n",
       "  \u001b[32m\"5\"\u001b[39m,\n",
       "  \u001b[32m\"8\"\u001b[39m,\n",
       "  \u001b[32m\"807\"\u001b[39m,\n",
       "  \u001b[32m\"808\"\u001b[39m,\n",
       "  \u001b[32m\"811\"\u001b[39m,\n",
       "  \u001b[32m\"814\"\u001b[39m,\n",
       "  \u001b[32m\"817\"\u001b[39m,\n",
       "  \u001b[32m\"818\"\u001b[39m,\n",
       "  \u001b[32m\"819\"\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seasonWindow = Window.partitionBy(\"year\")\n",
    "\n",
    "val lapCount = spark.read.parquet(\"../../data/parquet/lap_times.parquet\")\n",
    "    .join(races, Seq(\"raceId\"), \"right\")\n",
    "    .withColumn(\"lapsPerDriver\", count(col(\"lap\")).over(driverWindow))\n",
    "\n",
    "val (distinctDrivers, allLaps) = lapCount\n",
    "    .agg(\n",
    "        countDistinct(\"driverID\"),\n",
    "        count(col(\"lap\"))\n",
    "    ).as[(BigInt, BigInt)]\n",
    "    .collect()(0)\n",
    "\n",
    "val avgLapsThisPeriod = allLaps.toInt / distinctDrivers.toInt\n",
    "\n",
    "\n",
    "//filtrar con isInCollection vv es igual a hacer un join left-anti\n",
    "val experiencedDrivers = lapCount\n",
    "    .where(col(\"lapsPerDriver\") >= avgLapsThisPeriod)\n",
    "    .select(\"driverId\")\n",
    "    .distinct()\n",
    "    .as[String]\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90097503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmostConsistentDriver\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [driver: string, avgDiff: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mostConsistentDriver = spark.read.parquet(\"../../data/parquet/results.parquet\")\n",
    "    .join(races, Seq(\"raceId\"), \"right\")\n",
    "    .na.drop(Seq(\"fastestLapTime\"))\n",
    "//     .withColumn(\"fastestLapTime\", regexp_replace(col(\"fastestLapTime\"), \"[0-9]|[0-9][0-9]:[0-9][0-9]\\\\.[0-9][0-9][0-9]\", \".\"))\n",
    "//     .distinct().show()\n",
    "    .withColumn(\"fastestLapTimeMs\", lapTimeToMsUDF(col(\"fastestLapTime\")))\n",
    "    .withColumn(\"avgFastestLapMs\", avg(col(\"fastestLapTimeMs\")).over(driverWindow))\n",
    "    .dropDuplicates(\"driverId\")\n",
    "    .join(avg_lap_times, Seq(\"driverId\"), \"left\")\n",
    "    .withColumn(\"diffLapTimes\", abs('avgMs - 'avgFastestLapMs).cast(IntegerType))\n",
    "    .withColumn(\"avgDiff\", msToLapTimeUDF(col(\"diffLapTimes\").cast(IntegerType)))\n",
    "    .where(col(\"driverId\").isInCollection(experiencedDrivers))\n",
    "    .join(drivers, \"driverId\")\n",
    "    .withColumn(\"driver\", concat(col(\"forename\"), lit(\" \"), col(\"surname\")))\n",
    "    .select(\"driver\", \"avgDiff\")\n",
    "    .orderBy(\"avgDiff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2baf0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "// mostConsistentDriver.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "011347e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrun\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36maverage\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeTest\u001b[39m\n",
       "\u001b[36mres\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1466L\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run[A](code: => A): Long = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    System.currentTimeMillis() - start\n",
    "}\n",
    "\n",
    "def average(list: List[Long]):Long = list.sum / list.size.toLong\n",
    "\n",
    "def timeTest(f: => Long): Long = {\n",
    "    val list = (1 to 1).map(_ => f).toList\n",
    "    average(list)\n",
    "}\n",
    "\n",
    "val res = timeTest(run(mostConsistentDriver.collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e02092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf9253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf7360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b9edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
