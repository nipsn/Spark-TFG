{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90d563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/05/05 10:04:37 INFO SparkContext: Running Spark version 3.2.0\n",
      "22/05/05 10:04:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/05 10:04:37 INFO ResourceUtils: ==============================================================\n",
      "22/05/05 10:04:37 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/05/05 10:04:37 INFO ResourceUtils: ==============================================================\n",
      "22/05/05 10:04:37 INFO SparkContext: Submitted application: 08281ef8-428a-4ded-a90e-47881a889212\n",
      "22/05/05 10:04:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/05/05 10:04:37 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/05/05 10:04:37 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/05/05 10:04:37 INFO SecurityManager: Changing view acls to: oscar\n",
      "22/05/05 10:04:37 INFO SecurityManager: Changing modify acls to: oscar\n",
      "22/05/05 10:04:37 INFO SecurityManager: Changing view acls groups to: \n",
      "22/05/05 10:04:37 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/05/05 10:04:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oscar); groups with view permissions: Set(); users  with modify permissions: Set(oscar); groups with modify permissions: Set()\n",
      "22/05/05 10:04:37 INFO Utils: Successfully started service 'sparkDriver' on port 42875.\n",
      "22/05/05 10:04:37 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/05/05 10:04:37 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/05/05 10:04:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/05/05 10:04:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/05/05 10:04:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/05 10:04:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-38d0d790-655b-4bfb-8b15-5d54980c0bf8\n",
      "22/05/05 10:04:37 INFO MemoryStore: MemoryStore started with capacity 4.0 GiB\n",
      "22/05/05 10:04:37 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/05/05 10:04:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/05 10:04:38 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "22/05/05 10:04:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://netrunner:4041\n",
      "22/05/05 10:04:38 INFO Executor: Starting executor ID driver on host netrunner\n",
      "22/05/05 10:04:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32853.\n",
      "22/05/05 10:04:38 INFO NettyBlockTransferService: Server created on netrunner:32853\n",
      "22/05/05 10:04:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/05/05 10:04:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, netrunner, 32853, None)\n",
      "22/05/05 10:04:38 INFO BlockManagerMasterEndpoint: Registering block manager netrunner:32853 with 4.0 GiB RAM, BlockManagerId(driver, netrunner, 32853, None)\n",
      "22/05/05 10:04:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, netrunner, 32853, None)\n",
      "22/05/05 10:04:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, netrunner, 32853, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2d4314f9\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.2.0`\n",
    "import $ivy.`sh.almond::almond-spark:0.10.1`\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "      .master(\"local[4]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e588f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8acb1b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
       "\n",
       "// if you want to have the plots available without an internet connection:\n",
       "//init(offline=true)\n",
       "\n",
       "// restrict the output height to avoid scrolling in output cells\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.0`\n",
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "\n",
    "// if you want to have the plots available without an internet connection:\n",
    "//init(offline=true)\n",
    "\n",
    "// restrict the output height to avoid scrolling in output cells\n",
    "repl.pprinter() = repl.pprinter().copy(defaultHeight = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2be4ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950883cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mraces\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [raceId: string, year: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val races = spark.read.parquet(\"../../../data/parquetnopart/races.parquet\")\n",
    "    .select(\"raceId\", \"year\")\n",
    "    .where(col(\"year\") === 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706bd707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdrivers\u001b[39m: \u001b[32mDataFrame\u001b[39m = [driverId: string, driverRef: string ... 7 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val drivers = spark.read.parquet(\"../../../data/parquetnopart/drivers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cbb4438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriverWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@3c4040bb\n",
       "\u001b[36mseasonWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@1dd4ca2c\n",
       "\u001b[36mdriverRaceWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@605490fa\n",
       "\u001b[36mraceDriverLapWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@107fc405"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverWindow = Window.partitionBy(\"driverId\")\n",
    "val seasonWindow = Window.partitionBy(\"year\")\n",
    "val driverRaceWindow = Window.partitionBy(\"driverId\", \"raceId\")\n",
    "val raceDriverLapWindow = Window.partitionBy(\"driverId\", \"raceId\").orderBy(\"lap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7ccd54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriverStats\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [raceId: string, driverId: string ... 4 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverStats = spark.read.parquet(\"../../../data/parquetnopart/lap_times.parquet\")\n",
    "\n",
    "    .withColumn(\"position\", col(\"position\").cast(IntegerType)) \n",
    "    .withColumn(\"lap\", col(\"lap\").cast(IntegerType)) \n",
    "    .join(races, \"raceId\")\n",
    "\n",
    "    .withColumn(\"positionNextLap\", lead(col(\"position\"), 1).over(raceDriverLapWindow))\n",
    "    .withColumn(\"positionsGainedLap\", when(col(\"positionNextLap\") < col(\"position\") , abs(col(\"position\") - col(\"positionNextLap\"))).otherwise(0))\n",
    "    .withColumn(\"positionsLostLap\", when(col(\"positionNextLap\") > col(\"position\"), abs(col(\"position\") - col(\"positionNextLap\"))).otherwise(0))\n",
    "    .withColumn(\"positionsGained\", sum(col(\"positionsGainedLap\")).over(driverRaceWindow))\n",
    "    .withColumn(\"positionsLost\", sum(col(\"positionsLostLap\")).over(driverRaceWindow))\n",
    "    .withColumn(\"lapLeader\", when(col(\"position\") === 1, 1).otherwise(0))\n",
    "    .withColumn(\"lapsLed\", sum(col(\"lapLeader\")).over(driverWindow))\n",
    "    .withColumn(\"totalLaps\", sum(col(\"lapLeader\")).over(seasonWindow))\n",
    "    .withColumn(\"percLapsLed\", round(col(\"lapsLed\") / col(\"totalLaps\"), 2))\n",
    "    .select(\"raceId\", \"driverId\", \"positionsGained\", \"positionsLost\", \"lapsLed\", \"percLapsLed\")\n",
    "    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2468ec96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mresults\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [code: string, champPoints: bigint ... 12 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = spark.read.parquet(\"../../../data/parquetnopart/results.parquet\")\n",
    "\n",
    "    .withColumn(\"position\", col(\"position\").cast(IntegerType))    \n",
    "    .withColumn(\"grid\", col(\"grid\").cast(IntegerType))    \n",
    "    .withColumn(\"points\", col(\"points\").cast(IntegerType))\n",
    "\n",
    "    .join(races, \"raceId\")\n",
    "    .join(driverStats, Seq(\"raceId\", \"driverId\"), \"left\")\n",
    "    .join(drivers, \"driverId\")\n",
    "\n",
    "    .withColumn(\"podium\", when(col(\"position\") === 1 || col(\"position\") === 2 ||col(\"position\") === 3, lit(1)).otherwise(lit(0)))\n",
    "    .withColumn(\"averagePoints\", round(avg(col(\"points\")).over(driverWindow), 2))\n",
    "    .withColumn(\"maxAvgPoints\", max(col(\"averagePoints\")).over(seasonWindow))\n",
    "\n",
    "    .select(\n",
    "        col(\"code\"),\n",
    "        sum(col(\"points\")).over(driverWindow).as(\"champPoints\"),\n",
    "        col(\"averagePoints\"),\n",
    "        round(col(\"averagePoints\") / col(\"maxAvgPoints\"),2).as(\"pointPercent\"),\n",
    "        sum(col(\"podium\")).over(driverWindow).as(\"totalPodiums\"),        \n",
    "        round(avg(col(\"position\")).over(driverWindow), 2).as(\"avgPosition\"),\n",
    "        round(sum(col(\"podium\")).over(driverWindow) / count(col(\"podium\")).over(driverWindow), 2).as(\"podiumPercent\"),\n",
    "        round(avg(col(\"position\") - col(\"grid\")).over(driverWindow), 2).as(\"positionDelta\"),\n",
    "        round(avg(col(\"positionsLost\")).over(driverWindow), 2).as(\"avgPositionsLost\"),\n",
    "        round(avg(col(\"positionsGained\")).over(driverWindow), 2).as(\"avgPositionsWon\"),\n",
    "        sum(col(\"positionsLost\")).over(driverWindow).as(\"totalPositionsLost\"),\n",
    "        sum(col(\"positionsGained\")).over(driverWindow).as(\"totalPositionsWon\"),\n",
    "        col(\"lapsLed\"),\n",
    "        col(\"percLapsLed\")\n",
    "    )\n",
    "\n",
    "    .na.fill(0)\n",
    "    .dropDuplicates(Seq(\"code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf0f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "// results.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b95b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "// val driverCodes = results\n",
    "//     .sort(\"avgPosition\")\n",
    "//     .select(\"code\")\n",
    "//     .as[String]\n",
    "//     .collect()\n",
    "//     .toList\n",
    "\n",
    "// val positionDelta = results\n",
    "//     .sort(\"positionDelta\")\n",
    "//     .select(\"positionDelta\")\n",
    "//     .as[Double]\n",
    "//     .collect()\n",
    "//     .toList\n",
    "\n",
    "// val data = Seq(\n",
    "//   Bar(\n",
    "//       driverCodes,  \n",
    "//       positionDelta\n",
    "//   )\n",
    "// )\n",
    "\n",
    "// val layout = Layout( \n",
    "//   barmode = BarMode.Group\n",
    "// )\n",
    "\n",
    "// plot(data, layout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fd2d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrun\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36maverage\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeTest\u001b[39m\n",
       "\u001b[36mres\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2451L\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run[A](code: => A): Long = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    System.currentTimeMillis() - start\n",
    "}\n",
    "\n",
    "def average(list: List[Long]):Long = list.sum / list.size.toLong\n",
    "\n",
    "def timeTest(f: => Long): Long = {\n",
    "    val list = (1 to 1).map(_ => f).toList\n",
    "    average(list)\n",
    "}\n",
    "\n",
    "val res = timeTest(run(results.collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74fa6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
