{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc90d563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/05/04 21:45:05 INFO SparkContext: Running Spark version 3.2.0\n",
      "22/05/04 21:45:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/04 21:45:05 INFO ResourceUtils: ==============================================================\n",
      "22/05/04 21:45:05 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/05/04 21:45:05 INFO ResourceUtils: ==============================================================\n",
      "22/05/04 21:45:05 INFO SparkContext: Submitted application: 03502f9d-23c5-4eca-9d60-88dfd7818c97\n",
      "22/05/04 21:45:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/05/04 21:45:05 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/05/04 21:45:05 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/05/04 21:45:05 INFO SecurityManager: Changing view acls to: oscar\n",
      "22/05/04 21:45:05 INFO SecurityManager: Changing modify acls to: oscar\n",
      "22/05/04 21:45:05 INFO SecurityManager: Changing view acls groups to: \n",
      "22/05/04 21:45:05 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/05/04 21:45:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(oscar); groups with view permissions: Set(); users  with modify permissions: Set(oscar); groups with modify permissions: Set()\n",
      "22/05/04 21:45:05 INFO Utils: Successfully started service 'sparkDriver' on port 37657.\n",
      "22/05/04 21:45:05 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/05/04 21:45:05 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/05/04 21:45:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/05/04 21:45:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/05/04 21:45:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/04 21:45:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1bb492dd-224c-47f7-85b1-3a377e9229db\n",
      "22/05/04 21:45:05 INFO MemoryStore: MemoryStore started with capacity 4.0 GiB\n",
      "22/05/04 21:45:05 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/05/04 21:45:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/04 21:45:06 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "22/05/04 21:45:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://netrunner:4041\n",
      "22/05/04 21:45:06 INFO Executor: Starting executor ID driver on host netrunner\n",
      "22/05/04 21:45:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40413.\n",
      "22/05/04 21:45:06 INFO NettyBlockTransferService: Server created on netrunner:40413\n",
      "22/05/04 21:45:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/05/04 21:45:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, netrunner, 40413, None)\n",
      "22/05/04 21:45:06 INFO BlockManagerMasterEndpoint: Registering block manager netrunner:40413 with 4.0 GiB RAM, BlockManagerId(driver, netrunner, 40413, None)\n",
      "22/05/04 21:45:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, netrunner, 40413, None)\n",
      "22/05/04 21:45:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, netrunner, 40413, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@695b7668\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.2.0`\n",
    "import $ivy.`sh.almond::almond-spark:0.10.1`\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "      .master(\"local[4]\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2be4ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950883cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mraces\u001b[39m: \u001b[32mDataFrame\u001b[39m = [raceId: string, year: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val races = spark.read.parquet(\"../../../data/parquetnopart/races.parquet\")\n",
    "    .select(\"raceId\", \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f521722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriverInfo\u001b[39m: \u001b[32mDataFrame\u001b[39m = [driverId: string, fullName: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverInfo = spark.read.parquet(\"../../../data/parquetnopart/drivers.parquet\")\n",
    "    .select(col(\"driverId\"), \n",
    "        concat(col(\"forename\"), lit(\" \"), col(\"surname\")).alias(\"fullName\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbb4438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriverSeasonWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@8a59592"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverSeasonWindow = Window.partitionBy(\"driverId\", \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed996cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriverWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@1fbcf78e"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverWindow = Window.partitionBy(\"driverId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7efbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mseasonWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@656cb1a"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seasonWindow = Window.partitionBy(\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df9309",
   "metadata": {},
   "source": [
    "A calcular:\n",
    " - Porcentaje de salidas en la primera fila (primero o segundo)\n",
    " - Número de salidas en primera fila consecutivas\n",
    " - Porcentaje de victorias\n",
    " - Veces en el top 2 del campeonato mundial relativo al número de campeonatos en los que compitió\n",
    " - Porcentaje de vueltas lideradas\n",
    " - Posición media en parrilla (eliminar outliers)\n",
    " - Porcentaje de temporadas en las que ha hecho pole\n",
    " - Porcentaje de temporadas en las que ganó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41f7f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlastRace\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@592bef8f\n",
       "\u001b[36mlastRaces\u001b[39m: \u001b[32mDataFrame\u001b[39m = [raceId: string, year: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lastRace = Window.partitionBy(\"year\")\n",
    "\n",
    "val lastRaces = spark.read.parquet(\"../../../data/parquetnopart/races.parquet\")\n",
    "    .withColumn(\"round\", col(\"round\").cast(IntegerType))\n",
    "    .withColumn(\"max\", max(col(\"round\")).over(lastRace))\n",
    "    .where(col(\"round\") === col(\"max\"))\n",
    "    .select(\"raceId\", \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1893387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriverConstSeasonMap\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, driverId: string ... 1 more field]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverConstSeasonMap = spark.read.parquet(\"../../../data/parquetnopart/drivers_constr_season.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc6372f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mteammateWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@3cb270dd"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val teammateWindow = Window.partitionBy(\"year\", \"constructorId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e22e258f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriverDomination\u001b[39m: \u001b[32mDataFrame\u001b[39m = [year: string, driverId: string ... 2 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverDomination = spark.read.parquet(\"../../../data/parquetnopart/driver_standings.parquet\")\n",
    "    .join(lastRaces, Seq(\"raceId\"), \"right\")\n",
    "    .join(driverConstSeasonMap, Seq(\"driverId\", \"year\"), \"left\")\n",
    "    .withColumn(\"teamPointsPerc\", col(\"points\") / sum(col(\"points\")).over(teammateWindow))\n",
    "    .withColumn(\"bestOfTeam\", max(col(\"teamPointsPerc\")).over(teammateWindow))\n",
    "    .withColumn(\"dominatedTeammate\", when(col(\"teamPointsPerc\") === col(\"bestOfTeam\"), 1).otherwise(0))\n",
    "    .withColumn(\"dominationPerc\", round(sum(col(\"dominatedTeammate\")).over(driverWindow) / count(col(\"year\")).over(driverWindow) * 100, 2))\n",
    "    .withColumn(\"totalSeasons\", count(col(\"year\")).over(driverWindow))\n",
    "    .where(col(\"totalSeasons\") > 4)\n",
    "    .select(\"year\", \"driverId\", \"dominatedTeammate\", \"dominationPerc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "824f4459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdriverFilter\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [driverId: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val driverFilter = spark.read.parquet(\"../../../data/parquetnopart/results.parquet\")\n",
    "    .withColumn(\"finished\", when(col(\"statusId\") === 1, 1).otherwise(0))\n",
    "    .withColumn(\"numberOfFinishes\", sum(col(\"finished\")).over(driverWindow))\n",
    "    .where(col(\"numberOfFinishes\") < 5)\n",
    "    .select(\"driverId\")\n",
    "    .distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9121acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mraceConstructorWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@2440b853\n",
       "\u001b[36mseasonConstructorWindow\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@6cebe166"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val raceConstructorWindow = Window.partitionBy(\"raceId\", \"constructorId\")\n",
    "val seasonConstructorWindow = Window.partitionBy(\"year\", \"constructorId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd41cce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mteammateComparison\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [driverId: string, avgTopPosPerc: double ... 3 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val teammateComparison = spark.read.parquet(\"../../../data/parquetnopart/results.parquet\")\n",
    "    .join(driverFilter, Seq(\"driverId\"), \"leftanti\")\n",
    "//     .where(not(col(\"driverId\").isInCollection(driverFilter)))\n",
    "    .join(races, \"raceId\")\n",
    "\n",
    "\n",
    "    .withColumn(\"position\", col(\"position\").cast(IntegerType))\n",
    "    .withColumn(\"grid\", col(\"grid\").cast(IntegerType))\n",
    "    .na.fill(Map(\"position\" -> 100, \"grid\" -> 100))\n",
    "\n",
    "    .withColumn(\"grid\", when(col(\"grid\") === 0, 100).otherwise(col(\"grid\")))\n",
    "\n",
    "    .withColumn(\"topPos\", min(col(\"position\")).over(raceConstructorWindow))\n",
    "    .withColumn(\"constructorBestPos\", when(col(\"topPos\") === col(\"position\"), 1).otherwise(0))\n",
    "    .withColumn(\"topPosPerc\", sum(col(\"constructorBestPos\")).over(driverSeasonWindow) / \n",
    "                count(col(\"raceId\")).over(driverSeasonWindow) * 100)\n",
    "\n",
    "    .withColumn(\"constTopPosPerc\", max(col(\"topPosPerc\")).over(seasonConstructorWindow))\n",
    "    .withColumn(\"driverDomConstPos\", when(col(\"constTopPosPerc\") === col(\"topPosPerc\"), 1).otherwise(0))\n",
    "\n",
    "    .withColumn(\"topGrid\", min(col(\"grid\")).over(raceConstructorWindow))\n",
    "    .withColumn(\"constructorBestGridPos\", when(col(\"topGrid\") === col(\"grid\"), 1).otherwise(0))\n",
    "    .withColumn(\"topGridPerc\", sum(col(\"constructorBestGridPos\")).over(driverSeasonWindow) / \n",
    "                count(col(\"raceId\")).over(driverSeasonWindow) * 100)\n",
    "\n",
    "    .withColumn(\"constTopGridPerc\", max(col(\"topGridPerc\")).over(seasonConstructorWindow))\n",
    "    .withColumn(\"driverDomConstGrid\", when(col(\"constTopGridPerc\") === col(\"topGridPerc\"), 1).otherwise(0))\n",
    "\n",
    "    .dropDuplicates(\"driverId\", \"year\")\n",
    "\n",
    "    .withColumn(\"avgTopPosPerc\", avg(col(\"topPosPerc\")).over(driverWindow))\n",
    "    .withColumn(\"avgTopGridPerc\", avg(col(\"topGridPerc\")).over(driverWindow))\n",
    "    .withColumn(\"avgPosDom\", avg(col(\"driverDomConstPos\")).over(driverWindow))\n",
    "    .withColumn(\"avgGridDom\", avg(col(\"driverDomConstGrid\")).over(driverWindow))\n",
    "\n",
    "\n",
    "    .dropDuplicates(\"driverId\")\n",
    "\n",
    "//     .select(\"year\", \"driverId\",\"topPosPerc\", \"driverDomConstPos\",\"topGridPerc\", \"driverDomConstGrid\")\n",
    "    .select(\"driverId\", \"avgTopPosPerc\", \"avgTopGridPerc\", \"avgPosDom\", \"avgGridDom\")\n",
    "    .sort(col(\"avgPosDom\").desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f15486f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36maverageRank\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd14$Helper$$Lambda$4863/1341021492@2c2f7db3,\n",
       "  DoubleType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mNewInstance\u001b[39m(\n",
       "          class org.apache.spark.sql.catalyst.util.GenericArrayData,\n",
       "          \u001b[33mList\u001b[39m(\n",
       "            \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(interface scala.collection.Seq), true)\n",
       "          ),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          \u001b[33mArrayType\u001b[39m(IntegerType, false),\n",
       "          \u001b[32mNone\u001b[39m\n",
       "        ),\n",
       "        \u001b[33mUnresolvedMapObjects\u001b[39m(\n",
       "          org.apache.spark.sql.catalyst.ScalaReflection$$$Lambda$4876/289421155@5975e81b,\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mArrayType\u001b[39m(IntegerType, false)),\n",
       "          \u001b[33mSome\u001b[39m(interface scala.collection.Seq)\n",
       "        ),\n",
       "        scala.collection.Seq\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, DoubleType, false),\n",
       "      \u001b[33mAssertNotNull\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, DoubleType),\n",
       "          DoubleType,\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "      ),\n",
       "      Double\n",
       "    )\n",
       "...\n",
       "\u001b[36mres14_1\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mSparkUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd14$Helper$$Lambda$4863/1341021492@2c2f7db3,\n",
       "  DoubleType,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mExpressionEncoder\u001b[39m(\n",
       "        \u001b[33mNewInstance\u001b[39m(\n",
       "          class org.apache.spark.sql.catalyst.util.GenericArrayData,\n",
       "          \u001b[33mList\u001b[39m(\n",
       "            \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mObjectType\u001b[39m(interface scala.collection.Seq), true)\n",
       "          ),\n",
       "          \u001b[33mList\u001b[39m(),\n",
       "          true,\n",
       "          \u001b[33mArrayType\u001b[39m(IntegerType, false),\n",
       "          \u001b[32mNone\u001b[39m\n",
       "        ),\n",
       "        \u001b[33mUnresolvedMapObjects\u001b[39m(\n",
       "          org.apache.spark.sql.catalyst.ScalaReflection$$$Lambda$4876/289421155@5975e81b,\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, \u001b[33mArrayType\u001b[39m(IntegerType, false)),\n",
       "          \u001b[33mSome\u001b[39m(interface scala.collection.Seq)\n",
       "        ),\n",
       "        scala.collection.Seq\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mSome\u001b[39m(\n",
       "    \u001b[33mExpressionEncoder\u001b[39m(\n",
       "      \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, DoubleType, false),\n",
       "      \u001b[33mAssertNotNull\u001b[39m(\n",
       "        \u001b[33mUpCast\u001b[39m(\n",
       "          \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, DoubleType),\n",
       "          DoubleType,\n",
       "          \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "        ),\n",
       "        \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"scala.Double\\\"\"\u001b[39m)\n",
       "      ),\n",
       "      Double\n",
       "    )\n",
       "..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val averageRank = udf((cols: Seq[Int]) => {cols.sum / cols.size}: Double)\n",
    "spark.udf.register(\"averageRank\", averageRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deb52a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mresults\u001b[39m: \u001b[32mDataFrame\u001b[39m = [driverId: string, firstRowChance: double ... 28 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val results = spark.read.parquet(\"../../../data/parquetnopart/results.parquet\")\n",
    "    .join(driverFilter, Seq(\"driverId\"), \"left_anti\")\n",
    "    .join(races, \"raceId\")\n",
    "//     .join(driverInfo, \"driverId\")\n",
    "    \n",
    "    .withColumn(\"grid\", col(\"grid\").cast(IntegerType))\n",
    "    .withColumn(\"position\", col(\"position\").cast(IntegerType))\n",
    "\n",
    "    // porcentaje de salidas en primera fila (primero o segundo)\n",
    "    .withColumn(\"firstRowStart\", when(col(\"grid\") === 1  || col(\"grid\") === 2, 1).otherwise(0))\n",
    "    .withColumn(\"firstRowChance\", round(sum(col(\"firstRowStart\")).over(driverWindow) / count(col(\"firstRowStart\")).over(driverWindow), 4) * 100)\n",
    "\n",
    "    // porcentaje de temporadas en las que ha superado a su compañero de equipo\n",
    "    .join(driverDomination, Seq(\"driverId\", \"year\"), \"left\")\n",
    "    .join(teammateComparison, Seq(\"driverId\"), \"left\")\n",
    "\n",
    "    // posicion media en parrilla\n",
    "    .withColumn(\"avgGridStart\", round(avg(col(\"grid\")).over(driverWindow), 2))\n",
    "    // posicion media al acabar la carrera\n",
    "    .withColumn(\"avgFinish\", round(avg(col(\"position\")).over(driverWindow), 2))\n",
    "    \n",
    "    // porcentaje de temporadas con pole\n",
    "    .withColumn(\"pole\", when(col(\"grid\") === 1, 1).otherwise(0))\n",
    "    .withColumn(\"totalPolePositions\", sum(col(\"pole\")).over(driverWindow))\n",
    "    .withColumn(\"poleChance\", round(col(\"totalPolePositions\") / count(col(\"pole\")).over(driverWindow), 4) * 100)\n",
    "    .withColumn(\"polesPerSeason\", sum(col(\"pole\")).over(driverSeasonWindow))\n",
    "    .withColumn(\"poleChance\", round(col(\"totalPolePositions\") / count(col(\"raceId\")).over(driverWindow) * 100, 2))\n",
    "    .withColumn(\"hasPoleThisSeason\", when(col(\"polesPerSeason\") > 0, 1).otherwise(0))\n",
    "    .withColumn(\"percSeasonsWithPole\", round(sum(col(\"hasPoleThisSeason\")).over(driverWindow) / count(col(\"year\")).over(driverWindow), 4) * 100)\n",
    "    \n",
    "    // porcentaje de temporadas con victoria\n",
    "    .withColumn(\"win\", when(col(\"position\") === 1, 1).otherwise(0))\n",
    "    .withColumn(\"totalVictories\", sum(col(\"win\")).over(driverWindow))\n",
    "    .withColumn(\"victoryChance\", round(col(\"totalVictories\") / count(col(\"win\")).over(driverWindow), 4) * 100)\n",
    "    .withColumn(\"winsPerSeason\", sum(col(\"win\")).over(driverSeasonWindow))\n",
    "    .withColumn(\"hasWonThisSeason\", when(col(\"winsPerSeason\") > 0, 1).otherwise(0))\n",
    "    .withColumn(\"percSeasonsWithWins\", round(sum(col(\"hasWonThisSeason\")).over(driverWindow) / count(col(\"year\")).over(driverWindow), 4) * 100)\n",
    "\n",
    "    // porcentaje de podios\n",
    "    .withColumn(\"podium\", when(col(\"position\") === 1 || col(\"position\") === 2 ||col(\"position\") === 3, lit(1)).otherwise(lit(0)))\n",
    "    .withColumn(\"podiumChance\", round(sum(col(\"podium\")).over(driverWindow) / count(col(\"podium\")).over(driverWindow), 4) * 100)\n",
    "    \n",
    "\n",
    "    .dropDuplicates(\"driverId\")\n",
    "    .select(\"driverId\", \"firstRowChance\", \"avgGridStart\", \"avgFinish\", \"totalPolePositions\", \"poleChance\", \"percSeasonsWithPole\", \"percSeasonsWithWins\", \"podiumChance\", \"dominationPerc\", \"avgTopPosPerc\", \"avgTopGridPerc\", \"avgPosDom\", \"avgGridDom\")\n",
    "    .withColumn(\"rankFRC\", rank().over(Window.orderBy(col(\"firstRowChance\").desc)))\n",
    "    .withColumn(\"rankAGS\", rank().over(Window.orderBy(col(\"avgGridStart\").asc)))\n",
    "    .withColumn(\"rankAF\", rank().over(Window.orderBy(col(\"avgFinish\").asc)))\n",
    "    .withColumn(\"rankTPP\", rank().over(Window.orderBy(col(\"totalPolePositions\").desc)))\n",
    "    .withColumn(\"rankPSWP\", rank().over(Window.orderBy(col(\"percSeasonsWithPole\").desc)))\n",
    "    .withColumn(\"rankPSWW\", rank().over(Window.orderBy(col(\"percSeasonsWithWins\").desc)))\n",
    "    .withColumn(\"rankPC\", rank().over(Window.orderBy(col(\"podiumChance\").desc)))\n",
    "    .withColumn(\"rankDom\", rank().over(Window.orderBy(col(\"dominationPerc\").desc)))\n",
    "    .withColumn(\"rankPoleC\", rank().over(Window.orderBy(col(\"poleChance\").desc)))\n",
    "    .withColumn(\"rankPosPerc\", rank().over(Window.orderBy(col(\"avgTopPosPerc\").desc)))\n",
    "    .withColumn(\"rankGridPerc\", rank().over(Window.orderBy(col(\"avgTopGridPerc\").desc)))\n",
    "    .withColumn(\"rankPosDom\", rank().over(Window.orderBy(col(\"avgPosDom\").desc)))\n",
    "    .withColumn(\"rankGridDom\", rank().over(Window.orderBy(col(\"avgGridDom\").desc)))\n",
    "    .withColumn(\"stats\", averageRank(\n",
    "        array(col(\"rankFRC\"),\n",
    "            col(\"rankAGS\"),\n",
    "            col(\"rankAF\"),\n",
    "            col(\"rankTPP\"),\n",
    "            col(\"rankPSWP\"),\n",
    "            col(\"rankPSWW\"),\n",
    "            col(\"rankPC\"),\n",
    "            col(\"rankDom\"),\n",
    "            col(\"rankPoleC\"),\n",
    "            col(\"rankPosPerc\"),\n",
    "            col(\"rankGridPerc\"),\n",
    "            col(\"rankPosDom\"),\n",
    "            col(\"rankGridDom\")\n",
    "           )\n",
    "    ))\n",
    "    .withColumn(\"rank\", rank().over(Window.orderBy(col(\"stats\").asc)))\n",
    "    .sort(col(\"rank\").asc)\n",
    "    .join(driverInfo, \"driverId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "546c925d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mrun\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36maverage\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtimeTest\u001b[39m\n",
       "\u001b[36mres\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m3770L\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run[A](code: => A): Long = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    System.currentTimeMillis() - start\n",
    "}\n",
    "\n",
    "def average(list: List[Long]):Long = list.sum / list.size.toLong\n",
    "\n",
    "def timeTest(f: => Long): Long = {\n",
    "    val list = (1 to 1).map(_ => f).toList\n",
    "    average(list)\n",
    "}\n",
    "\n",
    "val res = timeTest(run(results.collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaba743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
